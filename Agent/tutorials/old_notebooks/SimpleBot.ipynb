{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd213a3",
   "metadata": {},
   "source": [
    "## Installations\n",
    "\n",
    "We first must install all required packages necessary for `agent` to run. Packages here are managed by `poetry`. For ease of use, we convert the poetry lock to a `requirements.txt` file. We then `pip install` the packages within this file. Any errors encountered during installation of any particular package can usually be resolved by manually pip installng that package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8335392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Create necessary directories for task files\n",
    "directories = {\n",
    "    \"config\": \"./../configs/task\",\n",
    "    \"simulator\": \"./../src/agent/environments\",\n",
    "    \"task\": \"./../src/agent/tasks\",\n",
    "    \"prompt_templates\": \"./../src/agent/prompts/templates/simplebot\",\n",
    "    \"llm_config\": \"./../configs/llm\",\n",
    "}\n",
    "\n",
    "for directory in directories.values():\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74975e60",
   "metadata": {},
   "source": [
    "# Building a Simple Bot with our Agent: A Step-by-Step Tutorial\n",
    "This tutorial will guide you through the process of setting up and deploying a simple bot using the Agent environment. We'll cover everything from configuring your project to implementing a custom task and testing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3730ff34",
   "metadata": {},
   "source": [
    "## Step 1: Specify Task Config\n",
    "Define your task configuration. This involves specifying the task's parameters and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec41a2d-a6bf-4d69-9825-48205a0a0286",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = \"\"\"\n",
    "# @package _global_\n",
    "agent:\n",
    "  pre_action_flow: ???\n",
    "  prompt_builder:\n",
    "    template_paths:\n",
    "      - simplebot\n",
    "      - default\n",
    "task:\n",
    "  _target_: src.agent.tasks.simplebot.SimpleBot\n",
    "  name: simplebot_test_env\n",
    "  description: \n",
    "  subtask: null\n",
    "  version: v0.1\n",
    "\"\"\"\n",
    "\n",
    "d = directories[\"config\"]\n",
    "with open(f\"{d}/simple_bot_task.yaml\", \"w\") as file:\n",
    "    # Write some text into the file\n",
    "    file.write(config_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75522ff5-7c08-46d1-957e-5070991e2e22",
   "metadata": {},
   "source": [
    "## Step 2: Implement a Custom Pybullet Environment\n",
    "This code defines a Python module that simulates a Panda robot arm in a PyBullet physics simulation environment. \n",
    "\n",
    "- MotionPlanner class: This class is responsible for generating smooth motion plans for the Panda robot arm to reach desired end-effector positions. It uses an optimization-based approach to find joint configurations (q-values) that minimize velocity and acceleration while reaching specified end-effector positions. The optimization problem is solved using the CasADi optimization framework with the IPOPT solver.\n",
    "\n",
    "- Robot class: This class represents the Panda robot in the simulation environment. It provides methods for controlling the robot, such as resetting joint positions, opening and closing the gripper, getting joint positions, setting joint targets, and moving the end effector to a desired goal position.\n",
    "\n",
    "- Environment class: This class sets up the simulation environment using PyBullet. It loads the Panda robot, a target box, and a ground plane. It provides methods for sampling box positions, getting box positions, setting the target box position, getting the gripper state, getting the end-effector position, calculating distances between the box and target, and resetting the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./../src/agent/environments/panda_simulator.py\n",
    "#%env LD_LIBRARY_PATH=\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import optas\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import numpy as np\n",
    "from pybullet_utils.bullet_client import BulletClient\n",
    "\n",
    "pb_path = pybullet_data.getDataPath()\n",
    "panda_urdf_filename = os.path.join(pb_path, \"franka_panda/panda.urdf\")\n",
    "\n",
    "end_effector_link = \"panda_grasptarget\"\n",
    "\n",
    "class MotionPlanner:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.robot = optas.RobotModel(panda_urdf_filename, time_derivs=[0, 1, 2])\n",
    "        self.name = self.robot.get_name()\n",
    "\n",
    "        self.duration = 5.0\n",
    "        T = 50\n",
    "        dt = self.duration / float(T - 1)\n",
    "        builder = optas.OptimizationBuilder(T, robots=self.robot, derivs_align=False)\n",
    "\n",
    "        qc = builder.add_parameter(\"qc\", self.robot.ndof)\n",
    "        pg = builder.add_parameter(\"pg\", 3)\n",
    "        use_mid_pose = builder.add_parameter(\"use_mid_pose\")\n",
    "        xg = optas.DM([1.0, 0.0, 0.0])\n",
    "        zg = optas.DM([0.0, 0.0, -1.0])\n",
    "        qn = optas.deg2rad([0.0, 20.0, 0.0, -90.0, 0.0, 120.0, 45.0, 0.0, 0.0])\n",
    "\n",
    "        builder.enforce_model_limits(self.name)\n",
    "\n",
    "        builder.initial_configuration(self.name, qc)\n",
    "        builder.initial_configuration(self.name, time_deriv=1)\n",
    "        builder.initial_configuration(self.name, time_deriv=2)\n",
    "        builder.fix_configuration(self.name, time_deriv=1)\n",
    "        builder.fix_configuration(self.name, time_deriv=2)\n",
    "        builder.integrate_model_states(self.name, time_deriv=1, dt=dt)\n",
    "        builder.integrate_model_states(self.name, time_deriv=2, dt=dt)\n",
    "\n",
    "        dQ = builder.get_model_states(self.name, time_deriv=1)\n",
    "        ddQ = builder.get_model_states(self.name, time_deriv=2)\n",
    "        builder.add_cost_term(\"minimize_velocity\", 20 * optas.sumsqr(dQ))\n",
    "        builder.add_cost_term(\"minimize_acceleration\", 20 * optas.sumsqr(ddQ))\n",
    "\n",
    "        t_mid = int(0.5 * T)\n",
    "        qmid = builder.get_model_state(self.name, t_mid, time_deriv=0)\n",
    "        builder.add_equality_constraint(\"mid\", use_mid_pose * qmid, use_mid_pose * qn)\n",
    "\n",
    "        for t in range(T):\n",
    "            q = builder.get_model_state(self.name, t)\n",
    "\n",
    "            Tf = self.robot.get_global_link_transform(end_effector_link, q)\n",
    "\n",
    "            dp = Tf[:3, 3] - pg\n",
    "            W = optas.diag([40.0, 40.0, 30.0 * float(t) / float(T - 1)])\n",
    "\n",
    "            builder.add_cost_term(f\"position_goal_{t}\", dp.T @ W @ dp)\n",
    "\n",
    "        builder.add_cost_term(\n",
    "            f\"orientation_goal_x_final\", 500 * optas.sumsqr(Tf[:3, 0] - xg)\n",
    "        )\n",
    "        builder.add_cost_term(\n",
    "            f\"orientation_goal_z_final\", 500 * optas.sumsqr(Tf[:3, 2] - zg)\n",
    "        )\n",
    "\n",
    "        builder.add_equality_constraint(\"final_position\", Tf[:3, 3], pg)\n",
    "\n",
    "        self.solver = optas.CasADiSolver(builder.build()).setup(\n",
    "            \"ipopt\", {\"ipopt.print_level\": 0, \"print_time\": 0, \"ipopt.sb\": \"yes\"}\n",
    "        )\n",
    "\n",
    "    def plan(self, qc, pg, use_mid_pose=True):\n",
    "        qc = np.concatenate((qc, [0.0, 0.0]))\n",
    "        self.solver.reset_parameters(\n",
    "            {\"pg\": pg, \"qc\": qc, \"use_mid_pose\": float(use_mid_pose)}\n",
    "        )\n",
    "        solution = self.solver.solve()\n",
    "        if not self.solver.did_solve():\n",
    "            return False, None\n",
    "        qfun = self.solver.interpolate(solution[f\"{self.name}/q\"], self.duration)\n",
    "        return True, qfun\n",
    "\n",
    "\n",
    "class Robot:\n",
    "\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.id = self.client.loadURDF(panda_urdf_filename, useFixedBase=1)\n",
    "        self.actuated_joint_indices = [0, 1, 2, 3, 4, 5, 6]\n",
    "        self.gripper_joint_indices = [9, 10]\n",
    "        self.ee_joint_index = 11\n",
    "        self.close_gripper()\n",
    "\n",
    "    def reset_joint_position(self, position):\n",
    "        for j, q in zip(self.actuated_joint_indices, position):\n",
    "            self.client.resetJointState(self.id, j, q)\n",
    "\n",
    "    def open_gripper(self):\n",
    "        self.client.setJointMotorControlArray(\n",
    "            self.id,\n",
    "            self.gripper_joint_indices,\n",
    "            p.POSITION_CONTROL,\n",
    "            [0.1, 0.1],\n",
    "        )\n",
    "\n",
    "    def close_gripper(self):\n",
    "        self.client.setJointMotorControlArray(\n",
    "            self.id,\n",
    "            self.gripper_joint_indices,\n",
    "            p.POSITION_CONTROL,\n",
    "            [0.0, 0.0],\n",
    "        )\n",
    "\n",
    "    def get_joint_position(self):\n",
    "        joint_states = self.client.getJointStates(self.id, self.actuated_joint_indices)\n",
    "        return [js[0] for js in joint_states]\n",
    "\n",
    "    def set_joint_target(self, position):\n",
    "        self.client.setJointMotorControlArray(\n",
    "            self.id,\n",
    "            self.actuated_joint_indices,\n",
    "            p.POSITION_CONTROL,\n",
    "            position,\n",
    "        )\n",
    "\n",
    "    def move_to_end_effector_goal(self, position, use_mid_pose=True):\n",
    "        # Temporarily redirect stderr to suppress specific PyBullet warnings\n",
    "        original_stderr = sys.stderr  # Save the original stderr\n",
    "        sys.stderr = open(os.devnull, 'w')  # Redirect stderr to null device\n",
    "\n",
    "        try:\n",
    "            planner = MotionPlanner()\n",
    "            qc = self.get_joint_position()\n",
    "            success, plan = planner.plan(qc, position, use_mid_pose=use_mid_pose)\n",
    "\n",
    "            # Restore stderr to its original state\n",
    "            sys.stderr.close()\n",
    "            sys.stderr = original_stderr\n",
    "\n",
    "            if not success:\n",
    "                print(\"Failed to plan joint motion.\")\n",
    "                return\n",
    "\n",
    "            t0 = time.time()\n",
    "            while True:\n",
    "                t = time.time() - t0\n",
    "                t_clipped = np.clip(t, 0, planner.duration)\n",
    "                q = plan(t_clipped)\n",
    "                self.set_joint_target(q[:-2])\n",
    "                if t > planner.duration:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            # Ensure stderr is restored even if an exception occurs\n",
    "            sys.stderr.close()\n",
    "            sys.stderr = original_stderr            \n",
    "\n",
    "            raise e  # Re-raise the exception to handle it elsewhere or let it propagate\n",
    "\n",
    "class Environment:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = BulletClient(\n",
    "            p.GUI,\n",
    "            options=\"--background_color_red=1 --background_color_blue=1 --background_color_green=1\",\n",
    "        )\n",
    "        self.client.configureDebugVisualizer(p.COV_ENABLE_GUI, 0)\n",
    "        self.client.resetDebugVisualizerCamera(\n",
    "            cameraDistance=1.5,\n",
    "            cameraYaw=45.0,\n",
    "            cameraPitch=-30.0,\n",
    "            cameraTargetPosition=(0.0, 0.0, 0.0),\n",
    "        )\n",
    "        self.client.setGravity(0.0, 0.0, -9.81)\n",
    "        self.box = self.client.loadURDF(\n",
    "            os.path.join(pb_path, \"cube.urdf\"),\n",
    "            self.sample_box_position(),\n",
    "            globalScaling=0.05,\n",
    "        )\n",
    "        self.set_target_box_position(self.sample_box_position())\n",
    "        self.client.loadURDF(os.path.join(pb_path, \"plane.urdf\"), useFixedBase=1)\n",
    "        self.robot = Robot(self.client)\n",
    "        self.client.setRealTimeSimulation(1)\n",
    "\n",
    "    def sample_box_position(self):\n",
    "        low = [0.4, -0.6, 0.1]\n",
    "        high = [0.7, 0.6, 0.1]\n",
    "        pos = np.random.uniform(low, high)\n",
    "        return pos\n",
    "\n",
    "    def get_box_position(self):\n",
    "        p, _ = self.client.getBasePositionAndOrientation(self.box)\n",
    "        return np.array(p)\n",
    "    \n",
    "    def set_target_box_position(self, position):\n",
    "        self.target_box_position = position\n",
    "    \n",
    "    def get_gripper_state(self):\n",
    "        gripper_pos = p.getJointState(self.robot.id, self.robot.gripper_joint_indices[0])[0]\n",
    "        if gripper_pos < 0.5:\n",
    "            return \"closed\"\n",
    "        else:\n",
    "            return \"open\"\n",
    "        \n",
    "    def get_ee_position(self):\n",
    "        return np.array(p.getLinkState(self.robot.id, self.robot.ee_joint_index)[0])\n",
    "    \n",
    "    def get_box_distance_to_target(self):\n",
    "        return np.linalg.norm(self.get_box_position() - self.target_box_position)\n",
    "    \n",
    "    def get_ee_distance_to_box(self):\n",
    "        return np.linalg.norm(self.get_box_position() - self.get_ee_position())\n",
    "        \n",
    "    def reset(self):\n",
    "        qn = np.deg2rad([0, 20, 0, -90, 0, 120, 45])\n",
    "        self.robot.set_joint_target(qn)\n",
    "        self.robot.reset_joint_position(qn)\n",
    "        self.client.resetBasePositionAndOrientation(\n",
    "            self.box, self.sample_box_position(), (0.0, 0.0, 0.0, 1.0)\n",
    "        )\n",
    "        self.set_target_box_position(self.sample_box_position())\n",
    "\n",
    "    def close(self):\n",
    "        if self.client.isConnected():\n",
    "            self.client.disconnect()\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9abc7",
   "metadata": {},
   "source": [
    "## Step 3: Implement a Custom Agent Task\n",
    "This code defines a SimpleBot task, which is a subclass of the Task class from agent-agent. The task involves controlling the simulated Panda robot arm to perform various actions based on feedback from LLM to manipulating a box in the previous defined environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965625c-930b-47a6-acd9-c31cc1474844",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./../src/agent/tasks/simplebot.py\n",
    "from typing import Any, Dict\n",
    "import numpy as np\n",
    "\n",
    "from agent.memory import MemKey\n",
    "from agent.tasks import ActionSpace\n",
    "from agent.tasks import Task\n",
    "from agent.utils import break_word_split\n",
    "from agent.environments.panda_simulator import Environment\n",
    "\n",
    "class SimpleBot(Task):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.observation_space = NotImplemented\n",
    "        self.action_space = ActionSpace.DISCRETE\n",
    "        self.args = kwargs\n",
    "        self.episode_counter = 0\n",
    "\n",
    "        self.env = Environment()\n",
    "\n",
    "        self.possible_actions = [\n",
    "            'open gripper',\n",
    "            'close gripper',\n",
    "            'move to pre grasp',\n",
    "            'move to box position',\n",
    "            'move to target box position',\n",
    "        ]\n",
    "\n",
    "    def reset(self, next_subtask: str | None = None):\n",
    "        self.done = False\n",
    "        self.env.reset()\n",
    "        return self._return_observation(self.possible_actions)\n",
    "\n",
    "    def _return_observation(self, available_actions: list[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Return the observation dictionary.\"\"\"\n",
    "\n",
    "        self.box_distance_to_target = np.round(self.env.get_box_distance_to_target(), 3)\n",
    "        self.ee_distance_to_box = np.round(self.env.get_ee_distance_to_box(), 3)\n",
    "        self.gripper_state = self.env.get_gripper_state()\n",
    "        obs = f\"\"\"\n",
    "        - Box distance to target: {self.box_distance_to_target}, \n",
    "        - End effector distance to box: {self.ee_distance_to_box}\n",
    "        - Gripper state: {self.gripper_state}\n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            MemKey.OBSERVATION: obs,\n",
    "            MemKey.AVAILABLE_ACTIONS: available_actions,\n",
    "        }\n",
    "    \n",
    "    def answer_parser(self, raw_response: str):\n",
    "        return break_word_split(\"Action\", raw_response)\n",
    "\n",
    "    def step(self, action):\n",
    "        available_actions = [a for a in self.possible_actions if a != action]\n",
    "        self.act(action)\n",
    "\n",
    "        # calculate reward \n",
    "        reward = 1\n",
    "        done = True if self.box_distance_to_target < 0.01 else False\n",
    "        if done:\n",
    "            self.open_gripper()\n",
    "\n",
    "        return self._return_observation(available_actions), reward, done\n",
    "\n",
    "    def open_gripper(self):\n",
    "        self.env.robot.open_gripper()\n",
    "\n",
    "    def close_gripper(self):\n",
    "        self.env.robot.close_gripper()\n",
    "\n",
    "    def move_to_pre_grasp(self):\n",
    "        pre_grasp = self.env.get_box_position() + np.array([0., 0., 0.1])\n",
    "        self.env.robot.move_to_end_effector_goal(pre_grasp, use_mid_pose=True)\n",
    "    \n",
    "    def move_to_box_position(self):\n",
    "        box_position = self.env.get_box_position()\n",
    "        self.env.robot.move_to_end_effector_goal(box_position, use_mid_pose=False)\n",
    "\n",
    "    def move_to_target_box_position(self):\n",
    "        target_box_position = self.env.target_box_position\n",
    "        self.env.robot.move_to_end_effector_goal(target_box_position, use_mid_pose=True)\n",
    "\n",
    "    def act(self, action):\n",
    "        action = action.lower()\n",
    "        if action == 'open gripper':\n",
    "            self.open_gripper()\n",
    "        elif action == 'close gripper':\n",
    "            self.close_gripper()\n",
    "        elif action == 'move to pre grasp':\n",
    "            self.move_to_pre_grasp()\n",
    "        elif action == 'move to box position':\n",
    "            self.move_to_box_position()\n",
    "        elif action == 'move to target box position':\n",
    "            self.move_to_target_box_position()\n",
    "        else:\n",
    "            return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf96684-d856-4479-b060-ee20e425fc3c",
   "metadata": {},
   "source": [
    "## Step 5: Define Prompt Template\n",
    "\n",
    "We now create a new directory to save custom prompt templates for our example task. Within /src/agent/prompts/example_task we create .jinja files to represent prompt templates. Below we define three prompts: \n",
    "*    system_prompt.jinja: a prompt to define the task to the LLM\n",
    " *   context_example.jinja: a prompt that serves to give the agent contextual information as to how the environment works, used to direct the agent to generate related content.\n",
    "  *  cot_prompt.jinja: used to get the agent to genrate a chain of thought and think step-by-step\n",
    "\n",
    "In this example, we want to generate a behaviour tree using an LLM., and structure prompts to represent this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97112ebc-8dc8-4bbf-93fd-20ed0b8f13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "    [[ SYSTEM ]]\n",
    "    You are an agent tasked with controlling a robotic arm to pick and place a small box.\n",
    "    The task of picking and placing the box can be broken down into a sequence of sub-tasks.\n",
    "    You will interact with the environment in a step-by-step manner. On each step, you are tasked with choosing an action with the aim of completing a sub-task. Detailed instructions are below:\n",
    "\n",
    "    * On each step, you will be given an observation of the current environment state. This includes the distance of the box to its target location, the state of the gripper on the arm, and the distance of the end effector to the box.\n",
    "    * You will also be given a list of available actions to take. You must choose a single action from this list to take. You cannot create new actions, this is a strict requirement.\n",
    "    * The robotic arm possesses a gripper. You may only grasp if the gripper is open.\n",
    "    * You must navigate the end effector to the target object before attempting to grasp it.\n",
    "    * Before attempting to grasp the box, you should move the arm to a pre-grasp position, in which the end effector is position is directly above the box.\n",
    "    * The gripper should be open before you attempt to move the end effector to the box position, in order to avoid pushing the box away.\n",
    "    * Occasionally the box may be in an awkward position that makes grasping difficult. If you fail to move the box, you should try again.\n",
    "    \n",
    "    {%- if cot_type in [\"zero_shot_cot\", \"few_shot_cot\"] %}\n",
    "    When asked for an action, you should think step by step, and then finish by providing an available action from the list.\n",
    "    Your response should be in the following format:\n",
    "    Thought: ...\n",
    "    Action: <action>\n",
    "    {%- else %}\n",
    "    When asked for an action, your response should be an available action from the list, using the following format:\n",
    "    Action: <action>\n",
    "    {%- endif %}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cot_prompt = \"\"\"\n",
    "    Now please think step by step, and then finish by providing an action that will help you complete the task.\n",
    "    Note: You can have multiple thoughts, but only generate a single action.\n",
    "    Answer in the format\n",
    "    Thought: ...\n",
    "    Action: <action>\n",
    "\"\"\"\n",
    "\n",
    "reflect_prompt = \"\"\"\n",
    "    {% include \"system_prompt.jinja\" %}\n",
    "    You will be given the history of a past experience in which you were placed in an environment and given a task to complete.\n",
    "    Do not summarize your environment or past actions, but rather think about the strategy and path you took to attempt to complete the task.\n",
    "    Devise a concise, new plan of action that accounts for any mistakes very briefly with reference to specific actions that you should have taken.\n",
    "    For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions.\n",
    "    Give your plan after seeing the past experience.\n",
    "    [[ USER ]]\n",
    "    {% include \"trajectory.jinja\" %}\n",
    "\n",
    "    Do not provide an action or a thought, but a new plan of action.\n",
    "    Answer in the format\n",
    "    New plan: <plan>\n",
    "\"\"\"\n",
    "\n",
    "trajectory_prompt = \"\"\"\n",
    "    {%- set observations = memory.retrieve_all({memory.mem_keys.OBSERVATION: 1.0})[::-1] -%}\n",
    "    {%- set actions = memory.retrieve_all({memory.mem_keys.EXTERNAL_ACTION: 1.0})[::-1] -%}\n",
    "\n",
    "    Here is what happened in this episode so far:\n",
    "    {%- for obs in observations -%}\n",
    "    {%- if loop.index0 > 0 %}\n",
    "    Action: {{actions[loop.index0-1]}}\n",
    "    {%- endif %}\n",
    "    {% if loop.index0 > 0 %}Observation: {% endif %}{{obs}}\n",
    "    {% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "context_example = \"\"\"\n",
    "    An example of solving a similar task is presented below:\n",
    "\n",
    "    Observation:\n",
    "    - Box distance to target: 0.387, \n",
    "    - End effector distance to box: 0.443\n",
    "    - Gripper state: closed\n",
    "    {%- if cot_type in [\"few_shot_cot\", \"react\"] %}\n",
    "    Thought: The box is not at the target, and the end effector is not near the box. I should move the arm to the pre grasp position.\n",
    "    {%- endif %}\n",
    "    Action: move to pre grasp.\n",
    "    Observation: \n",
    "    - Box distance to target: 0.387, \n",
    "    - End effector distance to box: 0.103\n",
    "    - Gripper state: closed\n",
    "    {%- if cot_type in [\"few_shot_cot\", \"react\"] %}\n",
    "    Thought: The box is not at the target, but the arm is in the pre grasp position. I should open the gripper.\n",
    "    {%- endif %}\n",
    "    Action: open gripper.\n",
    "    Observation: \n",
    "    - Box distance to target: 0.387, \n",
    "    - End effector distance to box: 0.103\n",
    "    - Gripper state: open\n",
    "    {%- if cot_type in [\"few_shot_cot\", \"react\"] %}\n",
    "    Thought: The gripper is open, the arm is in the pre grasp position, the box is not at the target position. I should move the arm to the box position.\n",
    "    {%- endif %}\n",
    "    Action: move to box position.\n",
    "    Observation: \n",
    "    - Box distance to target: 0.387, \n",
    "    - End effector distance to box: 0.02\n",
    "    - Gripper state: open\n",
    "    {%- if cot_type in [\"few_shot_cot\", \"react\"] %}\n",
    "    Thought: The gripper is open, the end effector is at the box, the box is not at the target position. I should close the gripper in order to grasp the box.\n",
    "    {%- endif %}\n",
    "    Action:  close gripper.\n",
    "    Observation: \n",
    "    - Box distance to target: 0.387, \n",
    "    - End effector distance to box: 0.02\n",
    "    - Gripper state: closed\n",
    "    {%- if cot_type in [\"few_shot_cot\", \"react\"] %}\n",
    "    Thought: The gripper is closed, the end effector is at the box, the box is not at the target position. I should move the arm to the target box position.\n",
    "    {%- endif %}\n",
    "    Action: move to target box position.\n",
    "\"\"\"\n",
    "\n",
    "d = directories[\"prompt_templates\"]\n",
    "\n",
    "with open(f\"{d}/system_prompt.jinja\", \"w\") as file:\n",
    "    file.write(system_prompt)\n",
    "\n",
    "with open(f\"{d}/cot_prompt.jinja\", \"w\") as file:\n",
    "    file.write(cot_prompt)\n",
    "\n",
    "with open(f\"{d}/reflect.jinja\", \"w\") as file:\n",
    "    file.write(reflect_prompt)\n",
    "\n",
    "with open(f\"{d}/trajectory.jinja\", \"w\") as file:\n",
    "    file.write(trajectory_prompt)\n",
    "\n",
    "with open(f\"{d}/context_example.jinja\", \"w\") as file:\n",
    "    file.write(context_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9343382-0ad1-44cb-a859-2f29383a8626",
   "metadata": {},
   "source": [
    "## Step 6: Config LLM server\n",
    "We must define an LLM backend for the agent to use. Within /configs/llm we create a file example_task_llm.yaml. Within this file, we specify the LLM and server IP that we want the agent to call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba1612-c1c2-4bb6-81ac-828556fa6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = \"\"\"\n",
    "_target_: src.agent.llms.llm.OpenAIBackend\n",
    "_partial_: true\n",
    "server_ip: http://127.0.0.1:8000/v1\n",
    "model_id: deepseek-coder-6.7b-instruct\n",
    "api_key: EMPTY \n",
    "\"\"\"\n",
    "\n",
    "d = directories[\"llm_config\"]\n",
    "with open(f\"{d}/deepseek_coder.yaml\", \"w\") as file:\n",
    "    file.write(llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab509511-e555-42e7-a575-d765cb5b8a00",
   "metadata": {},
   "source": [
    "## Step 7: Test Custom Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4ca54-a3fe-49b1-842f-e6a9dccd9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/agent/start.py task=simple_bot_task method=fs-cot-reflect llm@agent.llm=deepseek_coder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d2784",
   "metadata": {},
   "source": [
    "## Step 8: Clear files that has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3dcb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths_delete = [\n",
    "    \"./../src/agent/tasks/simplebot.py\",\n",
    "    \"./../src/agent/environments/panda_simulator.py\",\n",
    "]\n",
    "d = directories[\"config\"]\n",
    "file_paths_delete.append(f\"{d}/simple_bot_task.yaml\")\n",
    "d = directories[\"prompt_templates\"]\n",
    "file_paths_delete.append(f\"{d}/system_prompt.jinja\")\n",
    "file_paths_delete.append(f\"{d}/cot_prompt.jinja\")\n",
    "file_paths_delete.append(f\"{d}/reflect.jinja\")\n",
    "file_paths_delete.append(f\"{d}/trajectory.jinja\")\n",
    "file_paths_delete.append(f\"{d}/context_example.jinja\")\n",
    "d = directories[\"llm_config\"]\n",
    "file_paths_delete.append(f\"{d}/deepseek_coder.yaml\")\n",
    "for file_path in file_paths_delete:\n",
    "    try:\n",
    "        # Attempt to delete the file\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted file: {file_path}\")\n",
    "    except OSError as e:\n",
    "        # Handle any errors that occur during deletion\n",
    "        print(f\"Error deleting file {file_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
