{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new task in Agent\n",
    "\n",
    "This notebook will take you through all the steps required to create a task from scratch, so that you can do the same for your custom environments. We will look at creating everything needed for the GSM8k task. This task is already a part of the framework but we will recreate it from scratch so that you can follow the same steps for your own tasks.\n",
    "\n",
    "Note: Please make sure you have first installed all dependencies of Agent, following the installation guide!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create prompt templates\n",
    "\n",
    "Create a new directory under `../src/agent/prompts/templates` which will define custom prompt templates for your new task. These are the boilerplate .jinja files for any messages sent to the LLM. Here we look at creating templates for direct prompting in GSM8k.\n",
    "\n",
    "First we need a system prompt, which will be the system prompt passed on to the LLM and should define the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "template_dir = \"../src/agent/prompts/templates/example_gsm8k/\"\n",
    "os.makedirs(os.path.dirname(template_dir), exist_ok=True)\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "[[ SYSTEM ]]\n",
    "You are an expert mathematician. You are provided with mathematical questions that you have to answer.\n",
    "When asked for an answer, your response should use the following format:\n",
    "Answer: <answer>\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{template_dir}system_prompt.jinja\", \"w\") as file:\n",
    "    file.write(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to define a template for the prompting method we are using, here direct prompting. It should instruct the LLM as to what to answer, and in what format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_prompt = \"\"\"\n",
    "Now please answer the question.\n",
    "Answer in the format\n",
    "Answer: <answer>\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{template_dir}direct_prompt.jinja\", \"w\") as file:\n",
    "    file.write(direct_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to define how the trajectory of past observations (or possibly other content) should be presented to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_prompt = \"\"\"\n",
    "Question: {{memory.retrieve(memory.mem_keys.OBSERVATION)}}\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{template_dir}trajectory.jinja\", \"w\") as file:\n",
    "    file.write(trajectory_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `../src/agent/prompts/templates/default/` folder defines default templates which should come in handy for many tasks. In this case the default external_action.jinja template defines all the additional boilerplate we need, and will pull the direct prompt and trajectory prompt from the files we defined above when prompting the LLM for an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create a new Agent Task\n",
    "\n",
    "Here we create a new class inheriting from the Agent `Task`, which should contain at minimum the following crucial methods:\n",
    "- `reset()` - to reset the task and return the first observation\n",
    "- `step(action)` - to take a step in the task when given an action, returning an observation, a reward, and a `done` boolean\n",
    "- `answer_parser(raw_response)` - to parse the actual answer content needed from the raw LLM response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./../src/agent/tasks/example_gsm8k.py\n",
    "import math\n",
    "import re\n",
    "from typing import Any, Dict\n",
    "from datasets import load_dataset\n",
    "from agent.memory import MemKey\n",
    "from agent.tasks import ActionSpace\n",
    "from agent.tasks import DatasetOutOfBoundsException\n",
    "from agent.tasks import Task\n",
    "\n",
    "\n",
    "class GSM8K(Task):\n",
    "    def __init__(self, split: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.action_space = ActionSpace.CONTINUOUS\n",
    "        self.args = kwargs\n",
    "        self.dataset = load_dataset(\"gsm8k\", \"main\", split=split)\n",
    "        self.episode_counter = 0\n",
    "\n",
    "    def reset(self, next_subtask: str | None = None) -> Dict[str, str]:\n",
    "        \"\"\"Reset the environment and return the initial observation.\"\"\"\n",
    "\n",
    "        if next_subtask is not None:\n",
    "            self.episode_counter = int(next_subtask)\n",
    "\n",
    "        if self.episode_counter > len(self.dataset):\n",
    "            raise DatasetOutOfBoundsException(\n",
    "                \"The dataset index is not within dataset bounds. The end of the dataset may have been reached.\"\n",
    "            )\n",
    "\n",
    "        data = self.dataset[self.episode_counter]\n",
    "        self.answer = float(data[\"answer\"].split(\"\\n####\")[-1].replace(\",\", \"\"))\n",
    "        return self._return_observation(data)\n",
    "\n",
    "    def answer_parser(self, raw_response: str):\n",
    "        try:\n",
    "            proposed_answer = re.findall(r\"[-+]?(?:\\d*\\.*\\d+)\", raw_response.replace(\",\", \"\"))[-1]\n",
    "        except IndexError:\n",
    "            proposed_answer = \"\"\n",
    "        return proposed_answer\n",
    "\n",
    "    def step(self, action: str) -> tuple[dict, float, bool]:\n",
    "        \"\"\"Perform an action and return the next observation, reward, and done.\"\"\"\n",
    "\n",
    "        try:\n",
    "            reward = 1 if math.isclose(float(action), self.answer) else 0\n",
    "        except Exception:\n",
    "            reward = 0\n",
    "        self.episode_counter += 1\n",
    "        return {}, reward, True\n",
    "\n",
    "    def _return_observation(self, data: Dict[str, Any]) -> Dict[str, str]:\n",
    "        \"\"\"Return the observation for the current step.\"\"\"\n",
    "\n",
    "        return {MemKey.OBSERVATION: data[\"question\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a task configuration\n",
    "\n",
    "Create a new configuration file under `../configs/task/` which should define the task parameters, as well as the names of the directories to use for prompt templates (see [Step 1](#step-1-create-prompt-templates)), in priority order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = \"\"\"\n",
    "# @package _global_\n",
    "agent:\n",
    "  prompt_builder:\n",
    "    template_paths:\n",
    "      - gsm8k\n",
    "      - default\n",
    "\n",
    "task:\n",
    "  _target_: src.agent.tasks.example_gsm8k.GSM8K\n",
    "  name: gsm8k\n",
    "  subtask: null\n",
    "  version: v0.1\n",
    "  description:\n",
    "  split: test\n",
    "\n",
    "max_episodes: 3\n",
    "\"\"\"\n",
    "\n",
    "with open(\"../configs/task/example_gsm8k.yaml\", \"w\") as file:\n",
    "    file.write(config_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Running the task\n",
    "\n",
    "Assuming you are using an existing LLM configuration, say OpenChat-3.5, and prompting method, say direct prompting, the task can be run as follows. The task argument must be the name of the config file defined in [Step 3](#step-3-create-a-task-configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python ../src/agent/start.py task=example_gsm8k method=direct llm@agent.llm=hf/openchat_3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up files\n",
    "\n",
    "Here we clean up the files created by this notebook to avoid cluttering the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_files = [\n",
    "    f\"{template_dir}system_prompt.jinja\",\n",
    "    f\"{template_dir}direct_prompt.jinja\",\n",
    "    f\"{template_dir}trajectory.jinja\",\n",
    "    \"../src/agent/tasks/example_gsm8k.py\",\n",
    "    \"../configs/task/example_gsm8k.yaml\",\n",
    "]\n",
    "\n",
    "for file in delete_files:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "\n",
    "try:\n",
    "    os.rmdir(template_dir)\n",
    "except OSError as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
