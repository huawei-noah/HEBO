{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a new LLM to Agent\n",
    "\n",
    "This notebook will look at how to add support for a new LLM and using it with Agent. We add the OpenChat-3.5 model from HuggingFace in this example.\n",
    "\n",
    "Note: Please make sure you have first installed all dependencies of Agent, following the installation guide!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new LLM config\n",
    "\n",
    "The simplest way to add a new model is to use HuggingFace, either loading models from the hub or from a local path. Simply add a new config under `../configs/llm/hf/`, changing the model ID to a HuggingFace model or local model seen below. Also add the maximum context length of the model. \n",
    "\n",
    "Feel free to change the `model_id` to a locally downloaded model to avoid download times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = \"\"\"\n",
    "defaults:\n",
    "  - hf\n",
    "\n",
    "model_id: openchat/openchat-3.5-0106\n",
    "context_length: 8192\n",
    "tokenizer_kwargs: {}\n",
    "\"\"\"\n",
    "\n",
    "with open(\"../configs/llm/hf/example_openchat-3.5.yaml\", \"w\") as file:\n",
    "    file.write(llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the new LLM in Agent\n",
    "\n",
    "Assuming you are using an existing task, say gsm8k, and prompting method, say direct prompting, the new LLM can be run as follows. The `llm@agent.llm` argument must be the path to the config file defined above, starting from the `../configs/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! python ../src/agent/start.py task=gsm8k method=direct llm@agent.llm=hf/example_openchat-3.5 max_episodes=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up files\n",
    "\n",
    "Here we clean up the files created by this notebook to avoid cluttering the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.remove(\"../configs/llm/hf/example_openchat-3.5.yaml\")\n",
    "except OSError as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
