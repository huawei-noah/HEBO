import torch
from transformers import LogitsProcessor
from transformers import StoppingCriteria
from trl import DataCollatorForCompletionOnlyLM


def extract_collator_templates(tokenizer, return_tail=False):
    # The apply_chat_template functions should definitely return a user/agent/special token mask for this
    unique_token = tokenizer.unk_token

    chat_str = tokenizer.apply_chat_template(
        [{"role": "user", "content": unique_token}, {"role": "assistant", "content": unique_token}], tokenize=False
    )
    if tokenizer.chat_template is None:
        # llama-like template
        templates = chat_str.split(" " + unique_token + " ")
    else:
        # openchat-like template
        templates = chat_str.split(unique_token)
        templates = [t.replace("<s> ", "").strip() for t in templates]

    if return_tail:
        return templates
    else:
        return templates[:2]


def retrieve_training_tensors(tokenizer, prompt_messages: list[dict]):
    # remove last message if it's not generated by the assistant since it will be masked in the loss
    if prompt_messages[-1]["role"] != "assistant":
        prompt_messages = prompt_messages[:-1]

    query_tensors = tokenizer.apply_chat_template(prompt_messages, tokenize=True, return_tensors="pt")

    ignore_index = -100
    instruction_template, response_template = extract_collator_templates(tokenizer, return_tail=False)
    data_collator = DataCollatorForCompletionOnlyLM(
        instruction_template=instruction_template,
        response_template=response_template,
        tokenizer=tokenizer,
        ignore_index=ignore_index,
    )
    out = data_collator([query_tensors[0]])
    mask = out["labels"].squeeze(0) != ignore_index
    response_start = mask.int().argmax()
    query = out["input_ids"][0, :response_start]
    response = out["input_ids"][0, response_start:]
    response_masks = mask[response_start:]

    # can't pass tensor in multiprocessing without them being on cuda
    return query.tolist(), response.tolist(), response_masks.tolist()


class ReActLP(LogitsProcessor):
    def __init__(self, tokenizer) -> None:
        self.act_seq = tokenizer.encode(" ACTION: ", add_special_tokens=False)[:2]
        self.thought_seq = tokenizer.encode(" THOUGHT: ", add_special_tokens=False)[:2]
        self.jump_line_token = tokenizer.encode("\n", add_special_tokens=False)[-1]
        self.tokenizer = tokenizer

        assert self.act_seq[0] == self.thought_seq[0]

        self.should_react = 0
        self.picked_action = False

        # initialized in post_init
        self.not_eos = None
        self.second_token = None
        self.first_token = None

    def generation_start(self, size):
        self.should_react = 0
        self.picked_action = torch.zeros(size, dtype=torch.bool)

    def post_init(self, vocab_size):
        # tokenizer.vocab_size is not reliable, nor is get_vocab_size()
        # post_init based on the embedding size from the model
        self.first_token = torch.arange(vocab_size) != self.act_seq[0]
        self.second_token = (torch.arange(vocab_size) != self.act_seq[1]) * (
            torch.arange(vocab_size) != self.thought_seq[1]
        )
        self.not_eos = torch.arange(vocab_size) != self.tokenizer.eos_token_id

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        if self.first_token is None:
            self.post_init(scores.shape[-1])

        if self.should_react < 2:
            if self.should_react == 0:
                scores[:, self.first_token] = float("-inf")
            else:
                scores[:, self.second_token] = float("-inf")
        elif self.should_react == 2:
            self.picked_action = input_ids[:, -1] == self.act_seq[-1]
        else:
            for i in range(input_ids.shape[0]):
                if self.picked_action[i] and (input_ids[i, -self.should_react :] == self.jump_line_token).any():
                    scores[i, self.not_eos] = float("-inf")

        self.should_react += 1
        return scores


class OpenChatStoppingCriteria(StoppingCriteria):
    def __init__(self):
        self.should_stop = None

    def generation_start(self, size):
        self.should_stop = torch.zeros(size, dtype=torch.bool)

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        self.should_stop += (input_ids[:, -1] == 32000).cpu() + (input_ids[:, -1] == 2).cpu()
        return self.should_stop.all()
