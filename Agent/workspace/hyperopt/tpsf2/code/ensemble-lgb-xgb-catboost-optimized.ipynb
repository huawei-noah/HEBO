{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":25225,"databundleVersionId":1923495,"sourceType":"competition"}],"dockerImageVersionId":30061,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nThe purpose of this notebook is to predict the target by an Ensemble model composed of tree individual models\n\n- lightgbm\n- xgboost\n- catboost\n\nFeature Engineering followed basic practices that proved to work for GBM-style models for this competition\n\n- label encoding the cat variables\n- standard scaling to numeric variables\n\nParams for *xgboost* and *catboost* have been discovered via hyperparam search, using *hyperopt*. Params for *lightgbm* have been reused from https://www.kaggle.com/hiro5299834/tps-feb-2021-with-single-lgbm-tuned (they appeared to work better vs. the set of parameters I discovered in *hyperopt*-based search).\n\nWeight of lightgbm prediction was set to be a little higher then catboost and xgboost.\n\nThe well-thought software design of the Ensembling class was inspired by https://www.kaggle.com/kenkpixdev/ensemble-lgb-xgb-with-hyperopt","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nimport datetime as dt\nfrom typing import Tuple, List, Dict\n\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom hyperopt.pyll.base import scope\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main flow\nstart_time = dt.datetime.now()\nprint(\"Started at \", start_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read data\nin_kaggle = True\n\n\ndef get_data_file_path(is_in_kaggle: bool) -> Tuple[str, str, str]:\n    train_path = ''\n    test_path = ''\n    sample_submission_path = ''\n\n    if is_in_kaggle:\n        # running in Kaggle, inside the competition\n        train_path = '../input/tabular-playground-series-feb-2021/train.csv'\n        test_path = '../input/tabular-playground-series-feb-2021/test.csv'\n        sample_submission_path = '../input/tabular-playground-series-feb-2021/sample_submission.csv'\n    else:\n        # running locally\n        train_path = 'data/train.csv'\n        test_path = 'data/test.csv'\n        sample_submission_path = 'data/sample_submission.csv'\n\n    return train_path, test_path, sample_submission_path\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# get the training set and labels\ntrain_set_path, test_set_path, sample_subm_path = get_data_file_path(in_kaggle)\n\ntrain = pd.read_csv(train_set_path)\ntest = pd.read_csv(test_set_path)\ntarget = train.target\n\nsubm = pd.read_csv(sample_subm_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(df, encoder=None,\n               scaler=None, cols_to_drop=None,\n               cols_to_encode=None, cols_to_scale=None):\n    \"\"\"\n    Preprocess input data\n    :param df: DataFrame with data\n    :param encoder: encoder object with fit_transform method\n    :param scaler: scaler object with fit_transform method\n    :param cols_to_drop: columns to be removed\n    :param cols_to_encode: columns to be encoded\n    :param cols_to_scale: columns to be scaled\n    :return: DataFrame\n    \"\"\"\n\n    if encoder:\n        for col in cols_to_encode:\n            df[col] = encoder.fit_transform(df[col])\n\n    if scaler:\n        for col in cols_to_scale:\n            df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n\n    if cols_to_drop:\n        df = df.drop(cols_to_drop, axis=1)\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols = ['cat' + str(i) for i in range(10)]\ncont_cols = ['cont' + str(i) for i in range(14)]\n\ntrain = preprocess(train, encoder=LabelEncoder(), scaler=StandardScaler(),\n                  cols_to_drop=['id', 'target'], cols_to_encode=cat_cols,\n                  cols_to_scale=cont_cols)\n\n# encoder=LabelEncoder()\ntest = preprocess(test, encoder=LabelEncoder(), scaler=StandardScaler(),\n                 cols_to_drop=['id'], cols_to_encode=cat_cols,\n                 cols_to_scale=cont_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EnsembleModel:\n    def __init__(self, params):\n        \"\"\"\n        LGB + XGB + CatBoost model\n        \"\"\"\n        self.lgb_params = params['lgb']\n        self.xgb_params = params['xgb']\n        self.cat_params = params['cat']\n\n        self.lgb_model = LGBMRegressor(**self.lgb_params)\n        self.xgb_model = XGBRegressor(**self.xgb_params)\n        self.cat_model = CatBoostRegressor(**self.cat_params)\n\n    def fit(self, x, y, *args, **kwargs):\n        return (self.lgb_model.fit(x, y, *args, **kwargs),\n                self.xgb_model.fit(x, y, *args, **kwargs),\n               self.cat_model.fit(x, y, *args, **kwargs))\n\n    def predict(self, x, weights=[1.0, 1.0, 1.0]):\n        \"\"\"\n        Generate model predictions\n        :param x: data\n        :param weights: weights on model prediction, first one is the weight on lgb model\n        :return: array with predictions\n        \"\"\"\n        return (weights[0] * self.lgb_model.predict(x) +\n                weights[1] * self.xgb_model.predict(x) +\n                weights[2] * self.cat_model.predict(x)) / 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"since = time.time()\ncolumns = train.columns\n\n# ------------------------------------------------------------------------------\n# Parameters\n# ------------------------------------------------------------------------------\nN_FOLDS = 10\nN_ESTIMATORS = 30000\nSEED = 2021\nBAGGING_SEED = 48\n\n# ------------------------------------------------------------------------------\n# LightGBM: training and inference\n# ------------------------------------------------------------------------------\nlgb_params = {'random_state': SEED,\n          'metric': 'rmse',\n          'n_estimators': N_ESTIMATORS,\n          'n_jobs': -1,\n          'cat_feature': [x for x in range(len(cat_cols))],\n          'bagging_seed': SEED,\n          'feature_fraction_seed': SEED,\n          'learning_rate': 0.003899156646724397,\n          'max_depth': 99,\n          'num_leaves': 63,\n          'reg_alpha': 9.562925363678952,\n          'reg_lambda': 9.355810045480153,\n          'colsample_bytree': 0.2256038826485174,\n          'min_child_samples': 290,\n          'subsample_freq': 1,\n          'subsample': 0.8805303688019942,\n          'max_bin': 882,\n          'min_data_per_group': 127,\n          'cat_smooth': 96,\n          'cat_l2': 19\n          }\n\nensemble_params = {\n    \"lgb\" : lgb_params,\n    'xgb': {\n        'random_state': SEED,\n        'max_depth': 13,\n        'learning_rate': 0.020206705089028228,\n        'gamma': 3.5746731812451156,\n        'min_child_weight': 564,\n        'n_estimators': 8000,\n        'colsample_bytree': 0.5015940592112956,\n        'subsample': 0.6839489639112909,\n        'reg_lambda': 18.085502002853246,\n        'reg_alpha': 0.17532087359570606,\n        'objective': 'reg:squarederror',\n        'tree_method': 'gpu_hist',\n        'eval_metric': 'rmse',\n        'n_jobs': -1\n    },\n    'cat': {\n        'random_state': SEED,\n        'depth': 3.0,\n        'fold_len_multiplier': 1.1425259013471902,\n        'l2_leaf_reg': 7.567589781752637,\n        'leaf_estimation_backtracking': 'AnyImprovement',\n        'learning_rate': 0.25121635918496565,\n        'max_bin': 107.0,\n        'min_data_in_leaf': 220.0,\n        'random_strength': 3.2658690042589726,\n        'n_estimators': 8000,\n        'eval_metric': 'RMSE',\n    }\n}\n    \npreds = np.zeros(test.shape[0])\nkf = KFold(n_splits=N_FOLDS, random_state=22, shuffle=True)\nrmse = []\nn = 0\n\nfor trn_idx, test_idx in kf.split(train[columns], target):\n\n    X_tr, X_val=train[columns].iloc[trn_idx], train[columns].iloc[test_idx]\n    y_tr, y_val=target.iloc[trn_idx], target.iloc[test_idx]\n\n    model = EnsembleModel(ensemble_params)\n\n    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n\n    preds += model.predict(test[columns], weights=[1.1, 1.0, 0.9]) / kf.n_splits\n    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\n    \n    print(f\"Fold {n+1}, RMSE: {rmse[n]}\")\n    n += 1\n\n\nprint(\"Mean RMSE: \", np.mean(rmse))\nend_time = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(\n        end_time // 60, end_time % 60))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submit prediction\nsubm['target'] = preds\nsubm.to_csv(\"ensemble_model_lgb_xgb_cat_other_lgb_params.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('We are done. That is all, folks!')\nfinish_time = dt.datetime.now()\nprint(\"Finished at \", finish_time)\nelapsed = finish_time - start_time\nprint(\"Elapsed time: \", elapsed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}