{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Langugae Processing with Disaster Tweets: XGBoost and SVC Implementation\n\n<div style=\"text-align: center;\">\n    <img src=\"https://miro.medium.com/v2/resize:fit:600/1*iuoT4P9L802xZPg0x1oGgA.jpeg\" alt=\"House Prices\" width=\"600\" height=\"300\">\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nnltk.download('stopwords')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T15:00:21.009799Z","iopub.execute_input":"2024-08-06T15:00:21.010228Z","iopub.status.idle":"2024-08-06T15:00:23.511873Z","shell.execute_reply.started":"2024-08-06T15:00:21.01018Z","shell.execute_reply":"2024-08-06T15:00:23.510528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T15:01:12.939165Z","iopub.execute_input":"2024-08-06T15:01:12.939627Z","iopub.status.idle":"2024-08-06T15:01:12.99244Z","shell.execute_reply.started":"2024-08-06T15:01:12.939586Z","shell.execute_reply":"2024-08-06T15:01:12.991334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"print(train.isnull().sum())\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T15:01:22.917851Z","iopub.execute_input":"2024-08-06T15:01:22.918252Z","iopub.status.idle":"2024-08-06T15:01:22.926935Z","shell.execute_reply.started":"2024-08-06T15:01:22.918197Z","shell.execute_reply":"2024-08-06T15:01:22.925758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.describe())\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T15:00:51.926126Z","iopub.execute_input":"2024-08-06T15:00:51.92694Z","iopub.status.idle":"2024-08-06T15:00:51.945183Z","shell.execute_reply.started":"2024-08-06T15:00:51.926907Z","shell.execute_reply":"2024-08-06T15:00:51.94389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the distribution of the target variable\nplt.figure(figsize=(8, 6))\nsns.countplot(x='target', data=train, palette='viridis')\nplt.title('Distribution of Target Variable')\nplt.xlabel('Target')\nplt.ylabel('Count')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T15:00:29.969043Z","iopub.execute_input":"2024-08-06T15:00:29.970094Z","iopub.status.idle":"2024-08-06T15:00:30.232736Z","shell.execute_reply.started":"2024-08-06T15:00:29.970058Z","shell.execute_reply":"2024-08-06T15:00:30.231264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the length of tweets\ntrain['text_length'] = train['text'].apply(len)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(train['text_length'], kde=True, color='purple', bins=30)\nplt.title('Distribution of Tweet Lengths')\nplt.xlabel('Tweet Length')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T15:01:56.023324Z","iopub.execute_input":"2024-08-06T15:01:56.024094Z","iopub.status.idle":"2024-08-06T15:01:56.464228Z","shell.execute_reply.started":"2024-08-06T15:01:56.024057Z","shell.execute_reply":"2024-08-06T15:01:56.462935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize word count distribution\ntrain['word_count'] = train['text'].apply(lambda x: len(x.split()))\n\nplt.figure(figsize=(10, 6))\nsns.histplot(train['word_count'], kde=True, color='blue', bins=30)\nplt.title('Distribution of Word Counts in Tweets')\nplt.xlabel('Word Count')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T15:02:15.52682Z","iopub.execute_input":"2024-08-06T15:02:15.527305Z","iopub.status.idle":"2024-08-06T15:02:15.950651Z","shell.execute_reply.started":"2024-08-06T15:02:15.527269Z","shell.execute_reply":"2024-08-06T15:02:15.949644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize top 20 most frequent words in the tweets (excluding stop words)\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import Counter\n\nvectorizer = CountVectorizer(stop_words='english')\nword_count_vector = vectorizer.fit_transform(train['text'])\nword_counts = word_count_vector.toarray().sum(axis=0)\nvocab = vectorizer.get_feature_names_out() \n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T15:03:32.713472Z","iopub.execute_input":"2024-08-06T15:03:32.713995Z","iopub.status.idle":"2024-08-06T15:03:33.666472Z","shell.execute_reply.started":"2024-08-06T15:03:32.713953Z","shell.execute_reply":"2024-08-06T15:03:33.665225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_words_df = pd.DataFrame(sorted(list(zip(vocab, word_counts)), key=lambda x: x[1], reverse=True)[:20], \n                               columns=['Word', 'Count'])\n\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Count', y='Word', data=common_words_df, palette='coolwarm')\nplt.title('Top 20 Most Frequent Words')\nplt.xlabel('Count')\nplt.ylabel('Word')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T15:03:34.670181Z","iopub.execute_input":"2024-08-06T15:03:34.670986Z","iopub.status.idle":"2024-08-06T15:03:35.088412Z","shell.execute_reply.started":"2024-08-06T15:03:34.670952Z","shell.execute_reply":"2024-08-06T15:03:35.087432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess the Text Data","metadata":{}},{"cell_type":"code","source":"def preprocess(text):\n    text = text.lower() # Convert to lowercase\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) # Remove URLs\n    text = re.sub(r'<.*?>', '', text) # Remove HTML tags\n    text = re.sub(r'\\d+', '', text) # Remove digits\n    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text) # Remove punctuation\n    text = re.sub(r'\\n', '', text) # Remove newline characters\n    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra spaces\n    return text\n\ntrain['text'] = train['text'].apply(preprocess)\ntest['text'] = test['text'].apply(preprocess)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:35:31.134959Z","iopub.execute_input":"2024-08-06T02:35:31.135397Z","iopub.status.idle":"2024-08-06T02:35:31.490838Z","shell.execute_reply.started":"2024-08-06T02:35:31.135362Z","shell.execute_reply":"2024-08-06T02:35:31.489565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the Training Data","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train['text'], train['target'], test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:35:32.899099Z","iopub.execute_input":"2024-08-06T02:35:32.899525Z","iopub.status.idle":"2024-08-06T02:35:32.913365Z","shell.execute_reply.started":"2024-08-06T02:35:32.89949Z","shell.execute_reply":"2024-08-06T02:35:32.912155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vectorize the Text Data Using TF-IDF","metadata":{}},{"cell_type":"code","source":"tfidf = TfidfVectorizer(max_features=15000, stop_words=stopwords.words('english'))\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_val_tfidf = tfidf.transform(X_val)\nX_test_tfidf = tfidf.transform(test['text'])","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:35:35.777357Z","iopub.execute_input":"2024-08-06T02:35:35.77777Z","iopub.status.idle":"2024-08-06T02:35:36.201442Z","shell.execute_reply.started":"2024-08-06T02:35:35.777735Z","shell.execute_reply":"2024-08-06T02:35:36.199828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the Model","metadata":{}},{"cell_type":"code","source":"svc = SVC()\nsvc.fit(X_train_tfidf, y_train)\ny_pred_svc = svc.predict(X_val_tfidf)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:35:38.702731Z","iopub.execute_input":"2024-08-06T02:35:38.703225Z","iopub.status.idle":"2024-08-06T02:35:43.853657Z","shell.execute_reply.started":"2024-08-06T02:35:38.703187Z","shell.execute_reply":"2024-08-06T02:35:43.85231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\nxgb.fit(X_train_tfidf, y_train)\ny_pred_xgb = xgb.predict(X_val_tfidf)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:35:44.21812Z","iopub.execute_input":"2024-08-06T02:35:44.218572Z","iopub.status.idle":"2024-08-06T02:35:46.262731Z","shell.execute_reply.started":"2024-08-06T02:35:44.218535Z","shell.execute_reply":"2024-08-06T02:35:46.260742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Assuming you have these models trained already\nmodels = {\n    'Support Vector Classifier': svc,\n    'XGBoost': xgb\n}\n\n# Initialize variables to store the best model and accuracy\nbest_model = None\nbest_accuracy = 0\n\n# Evaluate each model and store the one with the best accuracy\nfor name, model in models.items():\n    predictions = model.predict(X_val_tfidf)\n    accuracy = accuracy_score(y_val, predictions)\n    print(f'{name} Accuracy: {accuracy}')\n    \n    if accuracy > best_accuracy:\n        best_accuracy = accuracy\n        best_model = model\n\nprint(f'\\nBest Model: {best_model} with Accuracy: {best_accuracy}')\n\n# Make predictions on the test data using the best model\ntest_predictions = best_model.predict(X_test_tfidf)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:35:49.592767Z","iopub.execute_input":"2024-08-06T02:35:49.593212Z","iopub.status.idle":"2024-08-06T02:35:52.297672Z","shell.execute_reply.started":"2024-08-06T02:35:49.593179Z","shell.execute_reply":"2024-08-06T02:35:52.296501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Create the submission DataFrame\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'target': test_predictions\n})\n\n# Save the submission file\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:35:58.318207Z","iopub.execute_input":"2024-08-06T02:35:58.318591Z","iopub.status.idle":"2024-08-06T02:35:58.3343Z","shell.execute_reply.started":"2024-08-06T02:35:58.318561Z","shell.execute_reply":"2024-08-06T02:35:58.332994Z"},"trusted":true},"execution_count":null,"outputs":[]}]}