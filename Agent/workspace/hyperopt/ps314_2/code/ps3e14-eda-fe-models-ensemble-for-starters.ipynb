{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":51959,"databundleVersionId":5624004,"sourceType":"competition"},{"sourceId":2462316,"sourceType":"datasetVersion","datasetId":1490445}],"dockerImageVersionId":30474,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom category_encoders import OneHotEncoder, MEstimateEncoder, GLMMEncoder, OrdinalEncoder\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold, KFold\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor, VotingRegressor, StackingRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.linear_model import PassiveAggressiveRegressor, ARDRegression\nfrom sklearn.linear_model import TheilSenRegressor, HuberRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_absolute_error, roc_auc_score, roc_curve\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import squareform\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\nsns.set_theme(style = 'white', palette = 'viridis')\npal = sns.color_palette('viridis')\n\npd.set_option('display.max_rows', 100)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-05-12T22:20:01.349785Z","iopub.execute_input":"2023-05-12T22:20:01.350259Z","iopub.status.idle":"2023-05-12T22:20:01.364009Z","shell.execute_reply.started":"2023-05-12T22:20:01.350224Z","shell.execute_reply":"2023-05-12T22:20:01.363183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(r'../input/playground-series-s3e14/train.csv')\ntest_1 = pd.read_csv(r'../input/playground-series-s3e14/test.csv')\norig_train = pd.read_csv(r'../input/wild-blueberry-yield-prediction-dataset/WildBlueberryPollinationSimulationData.csv')\n\ntrain.drop('id', axis = 1, inplace = True)\ntest = test_1.drop('id', axis = 1)\norig_train.drop('Row#', axis = 1, inplace = True)","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.042847,"end_time":"2023-04-14T03:31:18.325186","exception":false,"start_time":"2023-04-14T03:31:18.282339","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-12T22:20:02.006359Z","iopub.execute_input":"2023-05-12T22:20:02.006752Z","iopub.status.idle":"2023-05-12T22:20:02.123347Z","shell.execute_reply.started":"2023-05-12T22:20:02.006722Z","shell.execute_reply":"2023-05-12T22:20:02.122096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Knowing Your Data\n\n## Descriptive Statistics","metadata":{"papermill":{"duration":0.007001,"end_time":"2023-04-14T03:31:18.340047","exception":false,"start_time":"2023-04-14T03:31:18.333046","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train.head(10)","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.034425,"end_time":"2023-04-14T03:31:18.381669","exception":false,"start_time":"2023-04-14T03:31:18.347244","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-12T22:20:03.123559Z","iopub.execute_input":"2023-05-12T22:20:03.12396Z","iopub.status.idle":"2023-05-12T22:20:03.163563Z","shell.execute_reply.started":"2023-05-12T22:20:03.12393Z","shell.execute_reply":"2023-05-12T22:20:03.161657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"desc = train.describe().T\ndesc['nunique'] = train.nunique()\ndesc['%unique'] = desc['nunique'] / len(train) * 100\ndesc['null'] = train.isna().sum()\ndesc['type'] = train.dtypes\ndesc","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.056827,"end_time":"2023-04-14T03:31:18.507672","exception":false,"start_time":"2023-04-14T03:31:18.450845","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-12T22:20:03.166899Z","iopub.execute_input":"2023-05-12T22:20:03.168012Z","iopub.status.idle":"2023-05-12T22:20:03.282703Z","shell.execute_reply.started":"2023-05-12T22:20:03.167938Z","shell.execute_reply":"2023-05-12T22:20:03.281031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"desc = test.describe().T\ndesc['nunique'] = test.nunique()\ndesc['%unique'] = desc['nunique'] / len(train) * 100\ndesc['null'] = test.isna().sum()\ndesc['type'] = test.dtypes\ndesc","metadata":{"execution":{"iopub.status.busy":"2023-05-12T22:20:03.284898Z","iopub.execute_input":"2023-05-12T22:20:03.285311Z","iopub.status.idle":"2023-05-12T22:20:03.375563Z","shell.execute_reply.started":"2023-05-12T22:20:03.285277Z","shell.execute_reply":"2023-05-12T22:20:03.374112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"desc = orig_train.describe().T\ndesc['nunique'] = orig_train.nunique()\ndesc['%unique'] = desc['nunique'] / len(orig_train) * 100\ndesc['null'] = orig_train.isna().sum()\ndesc['type'] = orig_train.dtypes\ndesc","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.049846,"end_time":"2023-04-14T03:31:18.616805","exception":false,"start_time":"2023-04-14T03:31:18.566959","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-12T22:20:03.693257Z","iopub.execute_input":"2023-05-12T22:20:03.693682Z","iopub.status.idle":"2023-05-12T22:20:03.783589Z","shell.execute_reply.started":"2023-05-12T22:20:03.693649Z","shell.execute_reply":"2023-05-12T22:20:03.782233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Duplicates","metadata":{"papermill":{"duration":0.008253,"end_time":"2023-04-14T03:31:18.634556","exception":false,"start_time":"2023-04-14T03:31:18.626303","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(f'There are {train.duplicated(subset = list(train)[0:-1]).value_counts()[0]} non-duplicate values out of {train.count()[0]} rows in train dataset')\nprint(f'There are {test.duplicated().value_counts()[0]} non-duplicate values out of {test.count()[0]} rows in test dataset')\nprint(f'There are {orig_train.duplicated(subset = list(train)[0:-1]).value_counts()[0]} non-duplicate values out of {orig_train.count()[0]} rows in original train dataset')","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.029537,"end_time":"2023-04-14T03:31:18.67261","exception":false,"start_time":"2023-04-14T03:31:18.643073","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-12T22:20:05.169639Z","iopub.execute_input":"2023-05-12T22:20:05.170069Z","iopub.status.idle":"2023-05-12T22:20:05.727335Z","shell.execute_reply.started":"2023-05-12T22:20:05.170035Z","shell.execute_reply":"2023-05-12T22:20:05.725858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Key point**: There are row duplicates in train and test dataset. We can remove it from our train dataset, though it may have no effect due to how few they are.","metadata":{}},{"cell_type":"markdown","source":"# Adversarial Validation","metadata":{}},{"cell_type":"code","source":"def adversarial_validation(dataset_1 = train, dataset_2 = test, label = 'Train-Test'):\n\n    adv_train = dataset_1.drop('yield', axis = 1)\n    adv_test = dataset_2.copy()\n\n    adv_train['is_test'] = 0\n    adv_test['is_test'] = 1\n\n    adv = pd.concat([adv_train, adv_test], ignore_index = True)\n\n    adv_shuffled = adv.sample(frac = 1)\n\n    adv_X = adv_shuffled.drop('is_test', axis = 1)\n    adv_y = adv_shuffled.is_test\n\n    skf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n\n    val_scores = []\n    predictions = np.zeros(len(adv))\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(adv_X, adv_y)):\n    \n        adv_lr = XGBClassifier(random_state = 42)    \n        adv_lr.fit(adv_X.iloc[train_idx], adv_y.iloc[train_idx])\n        \n        val_preds = adv_lr.predict_proba(adv_X.iloc[val_idx])[:,1]\n        predictions[val_idx] = val_preds\n        val_score = roc_auc_score(adv_y.iloc[val_idx], val_preds)\n        val_scores.append(val_score)\n    \n    fpr, tpr, _ = roc_curve(adv['is_test'], predictions)\n    \n    plt.figure(figsize = (10, 10), dpi = 300)\n    sns.lineplot(x=[0, 1], y=[0, 1], linestyle=\"--\", label=\"Indistinguishable Datasets\")\n    sns.lineplot(x=fpr, y=tpr, label=\"Adversarial Validation Classifier\")\n    plt.title(f'{label} Validation = {np.mean(val_scores):.5f}', weight = 'bold', size = 17)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-12T22:20:06.640735Z","iopub.execute_input":"2023-05-12T22:20:06.641554Z","iopub.status.idle":"2023-05-12T22:20:06.654984Z","shell.execute_reply.started":"2023-05-12T22:20:06.641512Z","shell.execute_reply":"2023-05-12T22:20:06.653754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adversarial_validation()\nadversarial_validation(pd.concat([train, orig_train]), test, 'Combo Train-Test Validation')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-05-12T22:20:07.056079Z","iopub.execute_input":"2023-05-12T22:20:07.05734Z","iopub.status.idle":"2023-05-12T22:24:11.589528Z","shell.execute_reply.started":"2023-05-12T22:20:07.057282Z","shell.execute_reply":"2023-05-12T22:24:11.588235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Key points:**\n1. Train and test datasets validation results in ROC score of close to .5, therefore **we can trust our cross-validation.**\n2. Combined train and test datasets validation results in ROC score of close to .5, which is very far from competition dataset. Therefore, **we can include it in our training**.","metadata":{}},{"cell_type":"markdown","source":"# Distribution","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(4, 4, figsize = (10, 10), dpi = 300)\nax = ax.flatten()\n\nfor i, column in enumerate(test.columns):\n    sns.kdeplot(train[column], ax=ax[i], color=pal[0])    \n    sns.kdeplot(test[column], ax=ax[i], color=pal[2])\n    \n    ax[i].set_title(f'{column} Distribution', size = 7)\n    ax[i].set_xlabel(None)\n    \nfig.suptitle('Distribution of Feature\\nper Dataset\\n', fontsize = 24, fontweight = 'bold')\nfig.legend(['Train', 'Test'])\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Key points:** \n1. All features have similar distribution between training and test dataset.\n2. 13 out of 16 features are categorical","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 6), dpi = 300)\nsns.kdeplot(data = train, x = 'yield')\nplt.title('Target Distribution', weight = 'bold', size = 20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Key point**: It looks like we are having relatively normal distribution here.","metadata":{}},{"cell_type":"markdown","source":"# Correlation","metadata":{"papermill":{"duration":0.031508,"end_time":"2023-04-14T03:31:26.00169","exception":false,"start_time":"2023-04-14T03:31:25.970182","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def heatmap(dataset, label = None):\n    corr = dataset.corr(method = 'spearman')\n    plt.figure(figsize = (14, 10), dpi = 300)\n    mask = np.zeros_like(corr)\n    mask[np.triu_indices_from(mask)] = True\n    sns.heatmap(corr, mask = mask, cmap = 'viridis', annot = True, annot_kws = {'size' : 7})\n    plt.title(f'{label} Dataset Correlation Matrix\\n', fontsize = 25, weight = 'bold')\n    plt.show()","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.044524,"end_time":"2023-04-14T03:31:26.079498","exception":false,"start_time":"2023-04-14T03:31:26.034974","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heatmap(train, 'Train')\nheatmap(test, 'Test')","metadata":{"_kg_hide-input":true,"papermill":{"duration":2.669877,"end_time":"2023-04-14T03:31:28.781856","exception":false,"start_time":"2023-04-14T03:31:26.111979","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Key point**: There are so many features with very strong correlation that some of them are practically duplicates. We can remove them to make our model better. Let's try to see it with hierarchy tree this time.","metadata":{}},{"cell_type":"code","source":"def distance(data, label = ''):\n    #thanks to @sergiosaharovsky for the fix\n    corr = data.corr(method = 'spearman')\n    dist_linkage = linkage(squareform(1 - abs(corr)), 'complete')\n    \n    plt.figure(figsize = (10, 8), dpi = 300)\n    dendro = dendrogram(dist_linkage, labels=data.columns, leaf_rotation=90)\n    plt.title(f'Feature Distance in {label} Dataset', weight = 'bold', size = 22)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distance(train, 'Train')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Key points:** \n1. `MinOfUpperTRange`, `AverageOfUpperTRange`, `AverageOfLowerTRange`, `MaxOfLowerTRange`, `MaxOfUpperTRange`, and `MinOfLowerTRange` are practically duplicates so you can just keep one of them.\n2. `RainingDays` and `AverageRainingDays` are almost duplicate so you may also drop one of them.","metadata":{}},{"cell_type":"markdown","source":"# Preparation","metadata":{"papermill":{"duration":0.039506,"end_time":"2023-04-14T03:31:29.257052","exception":false,"start_time":"2023-04-14T03:31:29.217546","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X = train.copy()\ny = X.pop('yield')\n\nseed = 42\nsplits = 5\nk = KFold(n_splits = splits, random_state = seed, shuffle = True)\n\nnp.random.seed(seed)","metadata":{"_kg_hide-input":false,"papermill":{"duration":0.049654,"end_time":"2023-04-14T03:31:29.612968","exception":false,"start_time":"2023-04-14T03:31:29.563314","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base Models","metadata":{"papermill":{"duration":0.040817,"end_time":"2023-04-14T03:31:29.86419","exception":false,"start_time":"2023-04-14T03:31:29.823373","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def cross_val_score(model, cv = k, label = ''):\n    \n    X = train.copy()\n    y = X.pop('yield')\n    \n    #initiate prediction arrays and score lists\n    val_predictions = np.zeros((len(train)))\n    train_predictions = np.zeros((len(train)))\n    train_mae, val_mae = [], []\n    \n    #training model, predicting prognosis probability, and evaluating log loss\n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        \n        model.fit(X.iloc[train_idx], y.iloc[train_idx])\n\n        train_preds = model.predict(X.iloc[train_idx])\n        val_preds = model.predict(X.iloc[val_idx])\n                  \n        train_predictions[train_idx] += train_preds\n        val_predictions[val_idx] += val_preds\n        \n        train_score = mean_absolute_error(y.iloc[train_idx], train_preds)\n        val_score = mean_absolute_error(y.iloc[val_idx], val_preds)\n        \n        train_mae.append(train_score)\n        val_mae.append(val_score)\n    \n    print(f'Val MAE: {np.mean(val_mae):.5f} ± {np.std(val_mae):.5f} | Train MAE: {np.mean(train_mae):.5f} ± {np.std(train_mae):.5f} | {label}')\n    \n    return val_mae","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mae_list = pd.DataFrame()\n\nmodels = [\n    ('linear', LinearRegression()),\n    ('ridge', Ridge(random_state = seed)),\n    ('lasso', Lasso(random_state = seed, max_iter = 1000000)),\n    ('elastic', ElasticNet(random_state = seed, max_iter = 1000000)),\n    ('huber', HuberRegressor(max_iter = 1000000)),\n    ('ard', ARDRegression()),\n    ('passive', PassiveAggressiveRegressor(random_state = seed)),\n    ('theilsen', TheilSenRegressor(random_state = seed)),\n    ('linearsvm', LinearSVR(random_state = seed, max_iter = 1000000)),\n    ('mlp', MLPRegressor(random_state = seed, max_iter = 1000000)),\n    ('et', ExtraTreesRegressor(random_state = seed)),\n    ('rf', RandomForestRegressor(random_state = seed)),\n    ('xgb', XGBRegressor(random_state = seed, eval_metric = 'mae')),\n    ('lgb', LGBMRegressor(random_state = seed, objective = 'mae')),\n    ('dart', LGBMRegressor(random_state = seed, boosting_type = 'dart')),\n    ('cb', CatBoostRegressor(random_state = seed, objective = 'MAE', verbose = 0)),\n    ('gb', GradientBoostingRegressor(random_state = seed, loss = 'absolute_error')),\n    ('hgb', HistGradientBoostingRegressor(random_state = seed, loss = 'absolute_error')),\n    ('knn', KNeighborsRegressor())\n]\n\nfor (label, model) in models:\n     mae_list[label] = cross_val_score(model, label = label)","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8, 4), dpi = 300)\nsns.barplot(data = mae_list.reindex((mae_list).mean().sort_values().index, axis = 1), palette = 'viridis', orient = 'h')\nplt.title('MAE Comparison', weight = 'bold', size = 20)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Key points**;\n1. Linear regression can work as well as tree-based models here.\n2. Some tree-based models, especially non-gradient boosting ones, have a lot of overfitting.\n3. `CatBoostRegressor` gives the best result.","metadata":{}},{"cell_type":"markdown","source":"# Base Models 2.0 (With Post-Processing)\n\n@mattop has pointed out in [this topic](https://www.kaggle.com/competitions/playground-series-s3e14/discussion/407327) that we can post-process our prediction to make it consistent with the unique values of the `yield`.","metadata":{}},{"cell_type":"code","source":"def postprocessor(prediction):\n    #thanks to @mattop\n    unique_targets = np.unique(train['yield'])\n    return [min(unique_targets, key = lambda x: abs(x - pred)) for pred in prediction]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_val_score_2(model, cv = k, label = ''):\n    \n    X = train.copy()\n    y = X.pop('yield')\n    \n    #initiate prediction arrays and score lists\n    val_predictions = np.zeros((len(train)))\n    train_predictions = np.zeros((len(train)))\n    train_mae, val_mae = [], []\n    \n    #training model, predicting prognosis probability, and evaluating log loss\n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        \n        model.fit(X.iloc[train_idx], y.iloc[train_idx])\n\n        train_preds = postprocessor(model.predict(X.iloc[train_idx]))\n        val_preds = postprocessor(model.predict(X.iloc[val_idx]))\n                  \n        train_predictions[train_idx] += train_preds\n        val_predictions[val_idx] += val_preds\n        \n        train_score = mean_absolute_error(y.iloc[train_idx], train_preds)\n        val_score = mean_absolute_error(y.iloc[val_idx], val_preds)\n        \n        train_mae.append(train_score)\n        val_mae.append(val_score)\n    \n    print(f'Val MAE: {np.mean(val_mae):.5f} ± {np.std(val_mae):.5f} | Train MAE: {np.mean(train_mae):.5f} ± {np.std(train_mae):.5f} | {label}')\n    \n    return val_mae","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (label, model) in models:\n    mae_list[label] = cross_val_score_2(\n        model,\n        label = label\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8, 4), dpi = 300)\nsns.barplot(data = mae_list.reindex((mae_list).mean().sort_values().index, axis = 1), palette = 'viridis', orient = 'h')\nplt.title('MAE Comparison', weight = 'bold', size = 20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Key point:** It seems that we have a miniscule, but consistent improvement across our models on the MAE score.","metadata":{}},{"cell_type":"markdown","source":"# Base Model 3.0 (Postprocessing + Scaling)","metadata":{}},{"cell_type":"code","source":"for (label, model) in models:\n    mae_list[label] = cross_val_score_2(\n        Pipeline([\n            ('scale', StandardScaler()),\n            (label, model)]),\n        label = label\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8, 4), dpi = 300)\nsns.barplot(data = mae_list.reindex((mae_list).mean().sort_values().index, axis = 1), palette = 'viridis', orient = 'h')\nplt.title('MAE Comparison', weight = 'bold', size = 20)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Key point**: There is consistent improvement after we scale the features, especially on non-tree-based models.","metadata":{}},{"cell_type":"markdown","source":"# Ensemble\n\nNow let's try to build a simple average ensemble. For simplicity, we will only use LightGBM and CatBoost, which are the best 2 models here.","metadata":{}},{"cell_type":"code","source":"ensemble_models = [\n    ('lgb', LGBMRegressor(random_state = seed, objective = 'mae')),\n    ('cb', CatBoostRegressor(random_state = seed, objective = 'MAE', verbose = 0))\n]\n\nvoter = Pipeline([('scale', StandardScaler()), ('vote',VotingRegressor(ensemble_models))])\n\n_ = cross_val_score_2(voter, label = 'Voting Ensemble')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Key point**: It looks like our score has improved with the ensemble, from **343.11222** as the best score from our baseline CatBoost, to **341.60802** from simple average ensemble with scaling and post-processing.","metadata":{}},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"voter.fit(X, y)\nprediction = postprocessor(voter.predict(test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"papermill":{"duration":0.043219,"end_time":"2023-04-14T03:33:59.11743","exception":false,"start_time":"2023-04-14T03:33:59.074211","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test_1.drop(list(test_1.drop('id', axis = 1)), axis = 1, inplace = True)","metadata":{"papermill":{"duration":0.053599,"end_time":"2023-04-14T03:33:59.213081","exception":false,"start_time":"2023-04-14T03:33:59.159482","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_1['yield'] = prediction\ntest_1.to_csv('submission.csv', index = False)","metadata":{"papermill":{"duration":0.06033,"end_time":"2023-04-14T03:33:59.316152","exception":false,"start_time":"2023-04-14T03:33:59.255822","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thank you for reading!","metadata":{"papermill":{"duration":0.04172,"end_time":"2023-04-14T03:33:59.400672","exception":false,"start_time":"2023-04-14T03:33:59.358952","status":"completed"},"tags":[]}}]}