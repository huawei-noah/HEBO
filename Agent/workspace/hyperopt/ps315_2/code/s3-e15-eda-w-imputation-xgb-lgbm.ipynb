{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":51982,"databundleVersionId":5760919,"sourceType":"competition"}],"dockerImageVersionId":30474,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ“‚ Imports ðŸ“‚","metadata":{}},{"cell_type":"code","source":"# imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport optuna\n\nfrom sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer, LabelEncoder, OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor, StackingRegressor, BaggingRegressor\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom xgboost import XGBRegressor, plot_importance\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom imblearn.pipeline import Pipeline\n\nfrom itertools import combinations","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ“ˆ Exploratory Data Analysis ðŸ“Š","metadata":{}},{"cell_type":"markdown","source":"### Importing Data + a High-Level Look at the Data","metadata":{}},{"cell_type":"code","source":"# import the data\nall_data = pd.read_csv('data.csv')\n\n# separate data into train and submission sets based on blank target values\ntrain = all_data[all_data['x_e_out [-]'].isna() == False]\nsubmission = all_data[all_data['x_e_out [-]'].isna() == True]\n\n# get length of train and test datasets\nprint(f'\\nTrain dataset length: {train.shape[0]}')\nprint(f'Submission dataset length: {submission.shape[0]}\\n')\n\n# check for missing values\nprint(f'There are {int(train.isna().sum().sum())} missing feature values in the train set.')\nprint(f'There are {int(submission.isna().sum().sum())} missing feature values in the submission set.\\n')\n\n# check for duplicate rows\nn_duplicate_rows = len(train) - len(train.drop_duplicates())\nprint(f'There are {int(n_duplicate_rows)} duplicate rows in the train dataset.\\n')\n\n# quick high-level overview of dataset\npd.set_option('display.expand_frame_repr', False) # need this because there are so many features\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\ndisplay(train.head())\nprint('\\n\\n')\ndisplay(train.describe().round(decimals=2))","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ðŸ’¡Insights: First Glance\n- Train dataset is approximately twice as large as the submission dataset\n- The train dataset is missing a significant amount of data, averaging over one missing feature value per data point\n- The submission dataset is missing nearly twice as much data, averaging nearly two missing feature values per data point\n- There are no duplicate rows in the training dataset\n- D_h and chf_exp seem to have some significant outliers on the upper end","metadata":{}},{"cell_type":"markdown","source":"### Renaming Featurse + Creating lists of columns by feature type","metadata":{}},{"cell_type":"code","source":"# renaming columns to something more succinct and readable\ncolumn_renaming_dict = {'pressure [MPa]': 'pressure',\n                        'mass_flux [kg/m2-s]': 'mass_flux',\n                        'x_e_out [-]': 'x_e_out',\n                        'D_e [mm]': 'D_e',\n                        'D_h [mm]': 'D_h',\n                        'length [mm]': 'length',\n                        'chf_exp [MW/m2]': 'chf_exp'}\n\ntrain = train.rename(columns=column_renaming_dict)\nsubmission = submission.rename(columns=column_renaming_dict)\ndisplay(train.head())\n                        \n# creating groups by feature type\nfeatures = {'continuous': ['pressure', 'mass_flux', 'D_e', 'D_h', 'length', 'chf_exp'],\n            'categorical': ['author', 'geometry']}","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking target distribution","metadata":{}},{"cell_type":"code","source":"fix, ax = plt.subplots(figsize=(6, 6))\nsns.kdeplot(data=train, x='x_e_out', fill=True, ax=ax).set_title('Target Distribution on Train Set');\nax = np.ravel(ax)\nax[0].grid(visible=True)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a log transformation of the target\ntransformed_target = np.power(10, train[['x_e_out']]) - 1\n\n# plotting distribution\nfix, ax = plt.subplots(figsize=(6, 6))\nsns.kdeplot(data=transformed_target, x='x_e_out', fill=True, ax=ax).set_title('Transformed Target Distribution on Train Set [10^x - 1]');\nax = np.ravel(ax)\nax[0].grid(visible=True)\n\n# adding this to the dataframe\ntrain['log_x_e_out'] = transformed_target","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ðŸ’¡Insights: Target Distribution\n- The target distribution on the training data has some left skewness, but it does not seem too severe\n- Transforming the target with 10^x seems to make the distribution much more Gaussian. It may be worth looking into this to see if it helps the predictions","metadata":{}},{"cell_type":"markdown","source":"### Checking Feature Distribution","metadata":{}},{"cell_type":"code","source":"# plotting distribution of each continuous feature in train and test datasets\nfig, ax = plt.subplots(2, 3, figsize=(20, 10))\nax = np.ravel(ax)\npalette = sns.color_palette('coolwarm', 2)\n\nfor i, col in enumerate(features['continuous']):\n    sns.kdeplot(data=train, x=train[col], ax=ax[i], label='Train', color=palette[0], fill=True)\n    sns.kdeplot(data=submission, x=submission[col], ax=ax[i], label='Test', color=palette[1], fill=True)\n    ax[i].set_title(f'{col}', fontsize=12)\n    ax[i].legend(title='Dataset', loc='upper right', labels=['Train', 'Test'])\n    \nfig.suptitle('Continuous Feature Distributions (Train & Test)', fontsize=20);\nfig.tight_layout(pad=3)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating function to create a distribution histogram for each discrete value\ndef create_dist_barplot(train_df, test_df, feature_name, ax):\n    train_value_counts = pd.DataFrame(train_df.value_counts(feature_name, normalize=True))\n    train_value_counts['Distribution'] = ['Train'] * train_value_counts.shape[0]\n    test_value_counts = pd.DataFrame(test_df.value_counts(feature_name, normalize=True))\n    test_value_counts['Distribution'] = ['Test'] * test_value_counts.shape[0]\n    barplot_df = pd.concat([train_value_counts, test_value_counts], axis=0)\n    barplot_df = barplot_df.rename(columns={'proportion': 'Density'})\n    barplot_df = barplot_df.reset_index()\n    sns.barplot(data=barplot_df, x=feature_name, y='Density', hue='Distribution', ax=ax, palette='coolwarm')\n\n# plotting distribution of each integer feature in train and test datasets\nfig, ax = plt.subplots(1, 2, figsize=(20, 8))\nax = np.ravel(ax)\npalette = sns.color_palette('coolwarm', 2)\n\nfor i, col in enumerate(features['categorical']):\n    create_dist_barplot(train, submission, col, ax[i])\n    ax[i].set_title(f'{col}', fontsize=14)\n    \nfig.suptitle('Categorical Feature Distributions (Train & Test)', fontsize=20);\nfig.tight_layout(pad=1)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ðŸ’¡Insights: Feature Distributions:\n- The feature distributions between the train and test datasets seem to be extremely similar for both categorical and numerical features","metadata":{}},{"cell_type":"code","source":"# adjusting distributions of skewed features\ntrain['D_e'] = np.log1p(train['D_e'])\ntrain['D_h'] = np.log1p(train['D_h'])\ntrain['length'] = np.log1p(train['length'])\ntrain['chf_exp'] = np.log1p(train['chf_exp'])\n\nsubmission['D_e'] = np.log1p(submission['D_e'])\nsubmission['D_h'] = np.log1p(submission['D_h'])\nsubmission['length'] = np.log1p(submission['length'])\nsubmission['chf_exp'] = np.log1p(submission['chf_exp'])\n\n# plotting distribution of each continuous feature in train and test datasets\nfig, ax = plt.subplots(2, 3, figsize=(20, 10))\nax = np.ravel(ax)\npalette = sns.color_palette('coolwarm', 2)\n\nfor i, col in enumerate(features['continuous']):\n    sns.kdeplot(data=train, x=train[col], ax=ax[i], label='Train', color=palette[0], fill=True)\n    sns.kdeplot(data=submission, x=submission[col], ax=ax[i], label='Test', color=palette[1], fill=True)\n    ax[i].set_title(f'{col}', fontsize=12)\n    ax[i].legend(title='Dataset', loc='upper right', labels=['Train', 'Test'])\n    \nfig.suptitle('Continuous Feature Distributions (Train & Test)', fontsize=20);\nfig.tight_layout(pad=3)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Examining Feature Correlation","metadata":{"tags":[]}},{"cell_type":"code","source":"# calculating the raw correlation matrix\nraw_correlation = train[features['continuous'] + ['x_e_out']].corr()\n\n# only keeping the lower diagonal\ncorrelation = raw_correlation.copy()\nmask = np.zeros_like(correlation, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\ncorrelation[mask] = np.nan\n\n# plotting\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(correlation, annot=True, cmap='coolwarm', xticklabels=True, yticklabels=True, ax=ax, vmin=-1, vmax=1).set_title('Correlation Matrix', fontsize=20);","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# showing pairplot for continuous features\npairplot = sns.pairplot(data=train, vars=features['continuous'], diag_kind='kde');\npairplot.fig.suptitle('Pairplot for Continuous Features on Train Data', y=1.03, fontsize=20);","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ðŸ’¡Insights: Feature Correlation\n- After transforming some of the features, *D_e* and *D_h* are highly correlated. Before transformation, there weren't any highly correlated features\n- *D_h* is somewhat negatively correlated to pressure and positively correlated to *D_e*\n- *D_e* is also somewhat negatively correlated to pressure, which makes sense considering it is positively correlated with *D_h*\n- Looking at the pairplots, there is a strange correlation between *D_e* and *D_h*. They appear to have a perfect linear correlation with some random noise sprinkled in...","metadata":{}},{"cell_type":"markdown","source":"# ðŸ“ Feature Engineering ðŸ“","metadata":{}},{"cell_type":"markdown","source":"### Imputing missing categorical feature values and testing encoding technique","metadata":{}},{"cell_type":"code","source":"# add missing labels using the mode for each the categories\ntrain_imputed = train.copy(deep=True)\ntrain_imputed['author'] = train_imputed['author'].replace(np.nan, 'Thompson')\ntrain_imputed['geometry'] = train_imputed['geometry'].replace(np.nan, 'tube')\n\nsubmission_imputed = submission.copy(deep=True)\nsubmission_imputed['author'] = submission_imputed['author'].replace(np.nan, 'Thompson')\nsubmission_imputed['geometry'] = submission_imputed['geometry'].replace(np.nan, 'tube')\n\n# show the difference\ndisplay(train.head())\ndisplay(train_imputed.head())\n\n# create a categorical one-hot encoder\ncategorical_transformer = ColumnTransformer(transformers=[('onehot1', OneHotEncoder(sparse_output=False), ['author']),\n                                                          ('onehot2', OneHotEncoder(sparse_output=False), ['geometry']),\n                                                          ('passthrough', 'passthrough', features['continuous'])],\n                                            verbose_feature_names_out=False)\ncategorical_transformer.set_output(transform='pandas')\n\n# pass the data through the encoder\ntrain_cat_onehot_test = categorical_transformer.fit_transform(train_imputed)\ndisplay(train_cat_onehot_test.head())","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating Linear Regression based Imputer for D_e and D_h since they are highly correlated","metadata":{}},{"cell_type":"code","source":"# creating a custom transformer\nclass D_transformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.D_e_regressor = LinearRegression()\n        self.D_h_regressor = LinearRegression()\n        \n    def _impute_D_e(self, row):\n        D_e = row['D_e']\n        D_h = row['D_h']\n        if np.isnan(D_e) and np.isnan(D_h)==False:\n            D_e = self.D_e_regressor.predict(np.reshape(np.array(D_h), (-1, 1)))\n        return float(D_e)\n\n    def _impute_D_h(self, row):\n        D_e = row['D_e']\n        D_h = row['D_h']\n        if np.isnan(D_h) and np.isnan(D_e)==False:\n            D_h = self.D_h_regressor.predict(np.reshape(np.array(D_e), (-1, 1)))\n        return float(D_h)\n    \n    def fit(self, X, y=None):\n        # gathering D_e and D_h data where both are not NaN\n        complete_D_data = X[['D_h', 'D_e']]\n        filtered_D_data = complete_D_data[complete_D_data.isna().T.any() == False]\n\n        D_e_array = np.reshape(np.array(filtered_D_data['D_e']), (-1, 1))\n        D_h_array = np.reshape(np.array(filtered_D_data['D_h']), (-1, 1))\n\n        # fitting regressors for each based on complete data\n        self.D_e_regressor.fit(D_h_array, D_e_array)\n        self.D_h_regressor.fit(D_e_array, D_h_array)\n        \n        return self\n        \n    def transform(self, X, y=None):\n        X['D_e'] = X.apply(lambda row: self._impute_D_e(row), axis=1)\n        X['D_h'] = X.apply(lambda row: self._impute_D_h(row), axis=1)\n        \n        return X","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting \"train\" data into Train and Test Sets","metadata":{}},{"cell_type":"code","source":"target_name = 'x_e_out'\nfeatures_to_include = features['continuous'] + features['categorical']\n\n# splitting training data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(train_imputed[features_to_include],\n                                                    train_imputed[target_name],\n                                                    train_size=0.80,\n                                                    shuffle=True,\n                                                    random_state=1)\n\nprint(f'Size of X_train: {X_train.shape}\\nSize of y_train: {y_train.shape}')\nprint(f'Size of X_test: {X_test.shape}\\nSize of y_test: {y_test.shape}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building a baseline model and assessing features","metadata":{}},{"cell_type":"code","source":"# start with XGBoost\nX_baseline = X_train\ny_baseline = y_train\n\n# create a categorical one-hot encoder\ncategorical_transformer = ColumnTransformer(transformers=[('onehot1', OneHotEncoder(sparse_output=False), ['author']),\n                                                          ('onehot2', OneHotEncoder(sparse_output=False), ['geometry']),\n                                                          ('passthrough', 'passthrough', features['continuous'])],\n                                            verbose_feature_names_out=False)\ncategorical_transformer.set_output(transform='pandas')\n\n# create a regression imputer for D_h and D_e\nD_imputer = D_transformer()\n\n# create an imputer\nsimple_imputer = SimpleImputer(strategy='median', copy=True)\nsimple_imputer.set_output(transform='pandas')\n\n# create a baseline model to compare with\nbaseline_model = XGBRegressor(gamma=0.04, reg_lambda=0.04) # quickly add in some regularization by trial and error to prevent extreme overfitting\n\n# create the pipeline\nbaseline_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                              ('D_imputer', D_imputer),\n                              ('imputer', simple_imputer),\n                              ('regressor', baseline_model)])\ncv_results = cross_validate(baseline_pipeline, X_baseline, y_baseline, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add a feature of random noise to help judge feature importances\nX_baseline['random_noise'] = np.random.normal(size=X_baseline.shape[0])\n\n# create a categorical one-hot encoder that includes random noise\ncategorical_transformer_noise = ColumnTransformer(transformers=[('onehot1', OneHotEncoder(sparse_output=False), ['author']),\n                                                                ('onehot2', OneHotEncoder(sparse_output=False), ['geometry']),\n                                                                ('passthrough', 'passthrough', features['continuous'] + ['random_noise'])],\n                                            verbose_feature_names_out=False)\ncategorical_transformer_noise.set_output(transform='pandas')\n\n# create a baseline model to compare with\nfeature_importance_model = XGBRegressor(gamma=0.04, reg_lambda=0.04) # quickly add in some regularization by trial and error to prevent extreme overfitting\n\n# create the pipeline\nfeature_importance_pipeline = Pipeline([('cat_transformer', categorical_transformer_noise),\n                                        ('imputer', simple_imputer),\n                                        ('regressor', feature_importance_model)])\n\n# plotting feature importances\nfig, ax = plt.subplots(figsize=(10, 7))\nfeature_importance_pipeline.fit(X_baseline, y_baseline)\nplot_importance(feature_importance_model, ax=ax, importance_type='gain', show_values=False, xlabel='Gain', max_num_features=30);","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a partial dependence plot figure\nn_cols = 3\nn_rows = int(np.ceil(len(features['continuous']) / 3))\nfig, ax = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\nax = np.ravel(ax)\nfor i in range(len(ax)): # hiding any unused axes\n    if i >= len(features_to_include):\n        ax[i].set_visible(False)\nax = ax[0:len(features_to_include)]\n\n# transforming data\npdp_plot_data = categorical_transformer.fit_transform(X_train)\npdp_plot_data = simple_imputer.fit_transform(pdp_plot_data)\n\n# adding title\nfig.suptitle('Individual Conditional Expectation Plots for Features', fontsize=20);\nfig.tight_layout(pad=3)\n\n# plot PDP's and ICE's\nbaseline_pipeline.fit(X_baseline, y_baseline)\nPartialDependenceDisplay.from_estimator(baseline_model, pdp_plot_data, features['continuous'], n_cols=n_cols, kind='both', subsample=100, ax=ax)\n\n# adjusting y-axis values\nfor axis in ax:\n    axis.set_ylim([-0.15, 0.15])","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ðŸ’¡Insights: Baseline Model\n- Imputing missing categorical features with the mode seemed to work well\n- Imputing missing continuous features with the median seemed to work well also\n- The baseline XGBoost score seemed to perform fairly well and had much less variance than expected\n- The most important feature by far seems to be *chf_exp*, which out of all features was highest correlated to the target. *pressure*, *D_e*, *D_h*, *length*, and some *author* one-hots follow.\n- A random noise feature was added to use as a reference for how useful features are. All of the continuous features were above this threshold, but some *author* one-hots and all of the *geometry* one-hots were below the threshold.\n- *geometry* may be a good candidate as a feature to drop\n- The ICE plots show some odd behavior with the *D_e* and *D_h* features and their effect on the target.\n- *chf_exp* and *pressure* seem to have a visible negative correlation with the target. *length* seems to have a less apparent positive correlation. This matches the Pearson coefficients calculated earlier","metadata":{}},{"cell_type":"markdown","source":"### Testing alternative imputation techniques","metadata":{}},{"cell_type":"code","source":"# create a categorical one-hot encoder\ncategorical_transformer = ColumnTransformer(transformers=[('onehot1', OneHotEncoder(sparse_output=False), ['author']),\n                                                          ('onehot2', OneHotEncoder(sparse_output=False), ['geometry']),\n                                                          ('passthrough', 'passthrough', features['continuous'])],\n                                            verbose_feature_names_out=False)\ncategorical_transformer.set_output(transform='pandas')\n\n# create an imputer\nknn_imputer = KNNImputer(n_neighbors=3, weights='uniform', copy=True)\nknn_imputer.set_output(transform='pandas')\niterative_imputer = IterativeImputer(max_iter=10, imputation_order='descending')\niterative_imputer.set_output(transform='pandas')\n\n# create a baseline model to compare with\nimpute_trial_model = XGBRegressor(gamma=0.04, reg_lambda=0.04) # quickly add in some regularization by trial and error to prevent extreme overfitting\n\n# create the pipeline\nimpute_trial_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                                  ('D_imputer', D_imputer),\n                                  ('imputer', knn_imputer),\n                                  ('regressor', impute_trial_model)])\ncv_results = cross_validate(impute_trial_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trying XGBoost native missing values functionality\nimpute_trial_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                                  ('regressor', impute_trial_model)])\ncv_results = cross_validate(impute_trial_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ðŸ’¡Insights: Alternative Imputation Techniques\n- KNN Imputing seems to result in better performance for some values of K. The training takes significantly longer, though (on the order of 5-10 times as long). Running this through a hyperparameter optimizer may be problematic.\n- Iterative Imputing does not improve results after some experimentation with hyperparameters\n- XGBoost native functionality for missing values performed better than the baseline with statistical imputation techniques","metadata":{}},{"cell_type":"markdown","source":"### Exploring effect of dropping feature","metadata":{}},{"cell_type":"code","source":"# dropping any features that are not needed\nfeatures_to_exclude = ['geometry']\nall_features = features['continuous'] + features['categorical']\nfeatures_to_include = list(set(all_features) - set(features_to_exclude))\n\n# splitting training data into train and test sets\nX_train_drop, X_test_drop, y_train_drop, y_test_drop = train_test_split(train[features_to_include],\n                                                                        train[target_name],\n                                                                        train_size=0.80,\n                                                                        shuffle=True,\n                                                                        random_state=1)\n\nprint(f'Size of X_train: {X_train.shape}\\nSize of y_train: {y_train.shape}')\nprint(f'Size of X_test: {X_test.shape}\\nSize of y_test: {y_test.shape}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a categorical one-hot encoder\ncategorical_transformer_drop = ColumnTransformer(transformers=[('onehot1', OneHotEncoder(sparse_output=False), ['author']),\n                                                               ('passthrough', 'passthrough', features['continuous'])],\n                                                 verbose_feature_names_out=False)\ncategorical_transformer_drop.set_output(transform='pandas')\n\n# create a baseline model to compare with\ndrop_model = XGBRegressor(gamma=0.04, reg_lambda=0.04) # quickly add in some regularization by trial and error to prevent extreme overfitting\n\n# create the pipeline\ndrop_pipeline = Pipeline([('cat_transformer', categorical_transformer_drop),\n                          ('D_imputer', D_imputer),\n                          ('imputer', simple_imputer),\n                          ('regressor', drop_model)])\ncv_results = cross_validate(drop_pipeline, X_train_drop, y_train_drop, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting feature importances\nfig, ax = plt.subplots(figsize=(10, 7))\ndrop_pipeline.fit(X_train_drop, y_train_drop)\nplot_importance(drop_model, ax=ax, importance_type='gain', show_values=False, xlabel='Gain', max_num_features=30);","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ðŸ’¡Insights: Feature Refinement\n- Removing *geometry* feature slightly improved both score and variance. It is likely beneficial to completely remove this feature","metadata":{}},{"cell_type":"markdown","source":"### Trial to explore the results of using a transformed target","metadata":{}},{"cell_type":"code","source":"# transforming the target\ntransformed_target = np.power(10, train['x_e_out']) - 1\n\n# dropping any features that are not needed\nfeatures_to_include = features['continuous'] + features['categorical']\n\n# splitting training data into train and test sets\nX_train_trans, X_test_trans, y_train_trans, y_test_trans = train_test_split(train_imputed[features_to_include],\n                                                                            transformed_target,\n                                                                            train_size=0.80,\n                                                                            shuffle=True,\n                                                                            random_state=1)\n\nprint(f'Size of X_train: {X_train.shape}\\nSize of y_train: {y_train.shape}')\nprint(f'Size of X_test: {X_test.shape}\\nSize of y_test: {y_test.shape}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a function for the scoring metric (not an sklearn out-of-the-box metric)\ndef transformed_root_mean_squared_error(y_true, y_pred):\n    y_true_rev_trans = np.log10(y_true + 1)\n    y_pred_rev_trans = np.log10(y_pred + 1)\n    score = mean_squared_error(y_true_rev_trans, y_pred_rev_trans, squared=False)\n    \n    return score\n\n# create a scorer object to use in sklearn functions\ntransformed_rmse = make_scorer(score_func=transformed_root_mean_squared_error, greater_is_better=False)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a categorical one-hot encoder\ncategorical_transformer = ColumnTransformer(transformers=[('onehot1', OneHotEncoder(sparse_output=False), ['author']),\n                                                          ('onehot2', OneHotEncoder(sparse_output=False), ['geometry']),\n                                                          ('passthrough', 'passthrough', features['continuous'])],\n                                            verbose_feature_names_out=False)\ncategorical_transformer.set_output(transform='pandas')\n\n# create an imputer\nimputer = SimpleImputer(strategy='median', copy=True)\nimputer.set_output(transform='pandas')\n\n# create a baseline model to compare with\ntrans_model = XGBRegressor(gamma=0.07, reg_lambda=0.07) # quickly add in some regularization by trial and error to prevent extreme overfitting\n\n# create the pipeline\ntrans_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                           ('D_imputer', D_imputer),\n                           ('imputer', simple_imputer),\n                           ('regressor', trans_model)])\ncv_results = cross_validate(trans_pipeline, X_train_trans, y_train_trans, cv=10, scoring=transformed_rmse, return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ðŸ’¡Insights: Transformed Target\n- Transforming the target did seem to help the score slightly, although it was marginal and the regularization parameters had to be adjusted for the score to improve.\n- Might try investigating later","metadata":{}},{"cell_type":"markdown","source":"### Training CatBoost, Random Forest, and LightGBM models with baseline imputer to see how the performance compares","metadata":{}},{"cell_type":"code","source":"# resetting the categorical_transformer and imputer (getting rid of random noise feature and geometry)\ncategorical_transformer = ColumnTransformer(transformers=[('onehot1', OneHotEncoder(sparse_output=False), ['author']),\n                                                          ('passthrough', 'passthrough', features['continuous'])],\n                                            verbose_feature_names_out=False);\ncategorical_transformer.set_output(transform='pandas');","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a catboost model to compare with\ncb_model = CatBoostRegressor(silent=True, allow_writing_files=False)\n\n# create the pipeline\ncb_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                        ('D_imputer', D_imputer),\n                        ('imputer', simple_imputer),\n                        ('regressor', cb_model)])\n\ncv_results = cross_validate(cb_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a random forest model to compare with\nrf_model = RandomForestRegressor()\n\n# create the pipeline\nrf_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                        ('D_imputer', D_imputer),\n                        ('imputer', simple_imputer),\n                        ('regressor', rf_model)])\n\ncv_results = cross_validate(rf_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a LightGBM to compare with\nlgbm_model = LGBMRegressor()\n\n# create the pipeline\nlgbm_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                          ('D_imputer', D_imputer),\n                          ('imputer', simple_imputer),\n                          ('regressor', lgbm_model)])\n\ncv_results = cross_validate(lgbm_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ðŸ’¡Insights: Alternative Models\n- With default parameters, the CatBoost model had a better score and a comparable variance to the baseline model\n- With default parameters, the Random Forest model had much worse score, but a lower variance than the baseline model\n- With default parameters, the LightGBM model had an excellent score and a slightly worse variance than the baseline model\n- All of these models are good contenders for the final model, and could be used for an ensemble. Random forest might not make the cut","metadata":{}},{"cell_type":"markdown","source":"### Training CatBoost and LightGBM models with native functionality for missing values to see how the performance compares","metadata":{}},{"cell_type":"code","source":"# create the pipeline\ncb_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                        ('D_imputer', D_imputer),\n                        ('regressor', cb_model)])\n\ncv_results = cross_validate(cb_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the pipeline\nlgbm_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                          ('D_imputer', D_imputer),\n                          ('regressor', lgbm_model)])\n\ncv_results = cross_validate(lgbm_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ðŸ’¡Insights: Native Functionality for Missing Values\n- The CatBoost model had a comparable score and a better variance than the simple imputation model\n- The LightGBM model had a slightly better score and a better variance than the simple imputation model\n- Both of these boosting models should use their native functionality as it improves their performance","metadata":{}},{"cell_type":"markdown","source":"# ðŸ”§ Building and Optimizing an ML Model ðŸ”¨","metadata":{}},{"cell_type":"markdown","source":"### Optimizing Hyperparameters with Bayesian Optimization","metadata":{}},{"cell_type":"code","source":"# optimize XGBoost hyperparameters with optuna\ndef objective(trial):\n    # create the regressor object\n    regressor = XGBRegressor(n_estimators=trial.suggest_int('n_estimators', 70, 300),\n                             max_depth=trial.suggest_int('max_depth', 2, 8),\n                             min_child_weight=trial.suggest_float('min_child_weight', 0, 6),\n                             gamma=trial.suggest_float('gamma', 0.001, 6, log=True),\n                             learning_rate=trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n                             subsample=trial.suggest_float('subsample', 0.50, 1),\n                             colsample_bytree=trial.suggest_float('colsample_bytree', 0.5, 1),\n                             reg_lambda=trial.suggest_float('reg_lambda', 0.001, 5, log=True))\n\n    # create lists to store scores\n    train_scores = []\n    test_scores = []\n    \n    # create the pipeline\n    cv_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                            ('D_imputer', D_imputer),\n                            ('imputer', simple_imputer),\n                            ('regressor', regressor)])\n    \n    # begin cross-validation\n    cv_results = cross_validate(cv_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\n    mean_test_score = np.mean(cv_results['test_score'])\n    train_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n    return mean_test_score\n\n# begin optimization\noptuna.logging.set_verbosity(optuna.logging.WARNING) # won't print progress for every single trial\ncv_study = optuna.create_study(directions=['maximize'])\ncv_study.optimize(objective, n_trials=20)\n    \n# get the n best trials\nn = 10\nstudy_results_zipped = [(t.values[0], t.params) for t in cv_study.get_trials()]\nordered_study_results = sorted(study_results_zipped, key=lambda x: x[0], reverse=True)\nfor i, t in enumerate(ordered_study_results):\n    if i < n:\n        print(f'Trial {i} Results:')\n        print(f'Mean Test Score: {np.round(t[0], decimals=5)}')\n        print(t[1])\n        print('\\n')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimize CatBoost hyperparameters with optuna\ndef objective(trial):\n    # create the regressor object\n    regressor = CatBoostRegressor(iterations=trial.suggest_int('iterations', 100, 300),\n                                  depth=trial.suggest_int('depth', 4, 10),\n                                  l2_leaf_reg=trial.suggest_float('l2_leaf_reg', 0.001, 0.1, log=True),\n                                  random_strength=trial.suggest_float('random_strength', 0.0001, 1, log=True),\n                                  bagging_temperature=trial.suggest_float('bagging_temperature', 0, 1),\n                                  min_data_in_leaf=trial.suggest_int('min_data_in_leaf', 1, 100),\n                                  silent=True,\n                                  allow_writing_files=False)\n\n    # create lists to store scores\n    train_scores = []\n    test_scores = []\n    \n    # create imputer for catboost\n    cb_imputer = ColumnTransformer(transformers=[('passthrough', 'passthrough', ['author']),\n                                                 ('imputer', simple_imputer, features['continuous'])],\n                                   verbose_feature_names_out=False);\n    cb_imputer.set_output(transform='pandas');\n\n    # create the pipeline\n    cv_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                            ('D_imputer', D_imputer),\n                            ('imputer', simple_imputer),\n                            ('regressor', regressor)])\n    \n    # begin cross-validation\n    cv_results = cross_validate(cv_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\n    mean_test_score = np.mean(cv_results['test_score'])\n    train_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n    return mean_test_score\n\n# begin optimization\noptuna.logging.set_verbosity(optuna.logging.WARNING) # won't print progress for every single trial\ncv_study = optuna.create_study(directions=['maximize'])\ncv_study.optimize(objective, n_trials=20)\n\n# get the n best trials\nn = 10\nstudy_results_zipped = [(t.values[0], t.params) for t in cv_study.get_trials()]\nordered_study_results = sorted(study_results_zipped, key=lambda x: x[0], reverse=True)\nfor i, t in enumerate(ordered_study_results):\n    if i < n:\n        print(f'Trial {i} Results:')\n        print(f'Mean Test Score: {np.round(t[0], decimals=5)}')\n        print(t[1])\n        print('\\n')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimize Random Forest hyperparameters with optuna\ndef objective(trial):\n    # create the regressor object\n    regressor = RandomForestRegressor(n_estimators=trial.suggest_int('n_estimators', 60, 250),\n                                      max_depth=trial.suggest_categorical('max_depth', [10, 12, 15, 18, 20, 30, 40, 50, 70, 100, None]),\n                                      min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 30),\n                                      min_samples_split=trial.suggest_int('min_samples_split', 2, 20),\n                                      n_jobs=-1)\n\n    # create lists to store scores\n    train_scores = []\n    test_scores = []\n    \n    # create the pipeline\n    cv_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                            ('D_imputer', D_imputer),\n                            ('imputer', simple_imputer),\n                            ('regressor', regressor)])\n    \n    # begin cross-validation\n    cv_results = cross_validate(cv_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\n    mean_test_score = np.mean(cv_results['test_score'])\n    train_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n    return mean_test_score\n\n# begin optimization\noptuna.logging.set_verbosity(optuna.logging.WARNING) # won't print progress for every single trial\ncv_study = optuna.create_study(directions=['maximize'])\ncv_study.optimize(objective, n_trials=20)\n\n# get the n best trials\nn = 10\nstudy_results_zipped = [(t.values[0], t.params) for t in cv_study.get_trials()]\nordered_study_results = sorted(study_results_zipped, key=lambda x: x[0], reverse=True)\nfor i, t in enumerate(ordered_study_results):\n    if i < n:\n        print(f'Trial {i} Results:')\n        print(f'Mean Test Score: {np.round(t[0], decimals=5)}')\n        print(t[1])\n        print('\\n')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimize LightGBM hyperparameters with optuna\ndef objective(trial):\n    # create the regressor object\n    regressor = LGBMRegressor(n_estimators=trial.suggest_int('n_estimators', 70, 300),\n                              learning_rate=trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n                              num_leaves=trial.suggest_int('num_leaves', 20, 3000),\n                              max_depth=trial.suggest_int('max_depth', 3, 12),\n                              min_child_weight=trial.suggest_float('min_child_weight', 0.0005, 0.1, log=True),\n                              min_child_samples=trial.suggest_int('min_child_samples', 5, 50),\n                              subsample=trial.suggest_float('subsample', 0.5, 1),\n                              colsample_bytree=trial.suggest_float('colsample_bytree', 0.5, 1),\n                              reg_lambda=trial.suggest_float('reg_lambda', 0.001, 0.1, log=True))\n\n    # create lists to store scores\n    train_scores = []\n    test_scores = []\n    \n    # create the pipeline\n    cv_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                            ('D_imputer', D_imputer),\n                            ('imputer', simple_imputer),\n                            ('regressor', regressor)])\n    \n    # begin cross-validation\n    cv_results = cross_validate(cv_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\n    mean_test_score = np.mean(cv_results['test_score'])\n    train_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n    return mean_test_score\n\n# begin optimization\noptuna.logging.set_verbosity(optuna.logging.WARNING) # won't print progress for every single trial\ncv_study = optuna.create_study(directions=['maximize'])\ncv_study.optimize(objective, n_trials=20)\n\n# get the n best trials\nn = 10\nstudy_results_zipped = [(t.values[0], t.params) for t in cv_study.get_trials()]\nordered_study_results = sorted(study_results_zipped, key=lambda x: x[0], reverse=True)\nfor i, t in enumerate(ordered_study_results):\n    if i < n:\n        print(f'Trial {i} Results:')\n        print(f'Mean Test Score: {np.round(t[0], decimals=5)}')\n        print(t[1])\n        print('\\n')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating finalized models with optimized hyperparameters and checking CV scores","metadata":{}},{"cell_type":"code","source":"# create the XGBoost object\nxgb_final_1 = XGBRegressor(n_estimators=267,\n                           max_depth=7,\n                           min_child_weight=5.93313,\n                           gamma=0.002317,\n                           learning_rate=0.034267,\n                           subsample=0.60232,\n                           colsample_bytree=0.62122,\n                           reg_lambda=1.39154)\n\n# create the pipeline\nxgb_pipeline_1 = Pipeline([('cat_transformer', categorical_transformer),\n                           ('D_imputer', D_imputer),\n                           ('imputer', imputer),\n                           ('regressor', xgb_final_1)])\ncv_results = cross_validate(xgb_pipeline_1, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the XGBoost object\nxgb_final_2 = XGBRegressor(n_estimators=245,\n                           max_depth=7,\n                           min_child_weight=5.88154,\n                           gamma=0.0024124,\n                           learning_rate=0.035098,\n                           subsample=0.62636,\n                           colsample_bytree=0.61926,\n                           reg_lambda=1.30091)\n\n# create the pipeline\nxgb_pipeline_2 = Pipeline([('cat_transformer', categorical_transformer),\n                           ('D_imputer', D_imputer),\n                           ('imputer', imputer),\n                           ('regressor', xgb_final_2)])\ncv_results = cross_validate(xgb_pipeline_2, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a CatBoost object\ncb_final = CatBoostRegressor(iterations=500,\n                             depth=10,\n                             l2_leaf_reg=0.0718249,\n                             silent=True,\n                             allow_writing_files=False)\n\n# cb_final = CatBoostRegressor(silent=True,\n#                              allow_writing_files=False)\n\n# create the pipeline\ncb_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                        ('D_imputer', D_imputer),\n                        ('imputer', imputer),\n                        ('regressor', cb_final)])\ncv_results = cross_validate(cb_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a random forest object\nrf_final = RandomForestRegressor(n_estimators=141, \n                                 max_depth=15,\n                                 min_samples_leaf=12,\n                                 min_samples_split=5,\n                                 n_jobs=-1)\n\n# create the pipeline\nrf_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                        ('D_imputer', D_imputer),\n                        ('imputer', imputer),\n                        ('regressor', rf_final)])\ncv_results = cross_validate(rf_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the LightGBM object\nlgbm_final_1 = LGBMRegressor(n_estimators=176,\n                             learning_rate=0.031217,\n                             num_leaves=2681,\n                             max_depth=11,\n                             min_child_weight=0.03876,\n                             min_child_samples=47,\n                             subsample=0.61635,\n                             colsample_bytree=0.510339,\n                             reg_lambda=0.0082346)\n\n# create the pipeline\nlgbm_pipeline_1 = Pipeline([('cat_transformer', categorical_transformer),\n                            ('D_imputer', D_imputer),\n                            ('imputer', imputer),\n                            ('regressor', lgbm_final_1)])\ncv_results = cross_validate(lgbm_pipeline_1, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the LightGBM object\nlgbm_final_2 = LGBMRegressor(n_estimators=223,\n                             learning_rate=0.029477,\n                             num_leaves=2618,\n                             max_depth=10,\n                             min_child_weight=0.028960,\n                             min_child_samples=49,\n                             subsample=0.63466,\n                             colsample_bytree=0.52098,\n                             reg_lambda=0.007196)\n\n# create the pipeline\nlgbm_pipeline_2 = Pipeline([('cat_transformer', categorical_transformer),\n                            ('D_imputer', D_imputer),\n                            ('imputer', imputer),\n                            ('regressor', lgbm_final_2)])\ncv_results = cross_validate(lgbm_pipeline_2, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating an ensemble model","metadata":{"tags":[]}},{"cell_type":"code","source":"# creating a voting ensemble from the models\nvoting_model = VotingRegressor(estimators=[('xgb_1', xgb_final_1), ('xgb_2', xgb_final_2), ('lgbm_1', lgbm_final_1), ('lgbm_2', lgbm_final_2)])\nvoting_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                            ('D_imputer', D_imputer),\n                            ('imputer', imputer),\n                            ('regressor', voting_model)])\n\n# begin cross-validation\ncv_results = cross_validate(voting_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')\n\n# fit the pipeline to the training data and report score on test data\nvoting_pipeline.fit(X_train, y_train)\ny_test_pred = voting_pipeline.predict(X_test)\nmodel_score = mean_squared_error(y_test, y_test_pred, squared=False)\nprint(f'Test Data Score: {np.round(model_score, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a stacked ensemble from the models\nstacked_model = StackingRegressor(estimators=[('xgb_1', xgb_final_1), ('xgb_2', xgb_final_2), ('lgbm_1', lgbm_final_1), ('lgbm_2', lgbm_final_2)], final_estimator=BayesianRidge())\nstacked_pipeline = Pipeline([('cat_transformer', categorical_transformer),\n                             ('D_imputer', D_imputer),\n                             ('imputer', imputer),\n                             ('regressor', stacked_model)])\n\n# begin cross-validation\ncv_results = cross_validate(stacked_pipeline, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\nmean_test_score = np.mean(cv_results['test_score'])\ntrain_test_score_rmse = np.std(cv_results['test_score']) # helpful to measure variance\n\n# print scoring metrics\nprint('Test Scores on K-Folds: ' + str(np.round(cv_results['test_score'], decimals=3)))\nprint('Train Scores on K-Folds: ' + str(np.round(cv_results['train_score'], decimals=3)))\nprint(f'Mean Test Score: {np.round(mean_test_score, decimals=5)}')\nprint(f'Test Score K-Fold Std: {np.round(train_test_score_rmse, decimals=5)}')\n\n# fit the pipeline to the training data and report score on test data\nstacked_pipeline.fit(X_train, y_train)\ny_test_pred = stacked_pipeline.predict(X_test)\nmodel_score = mean_squared_error(y_test, y_test_pred, squared=False)\nprint(f'Test Data Score: {np.round(model_score, decimals=5)}')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ“¦ Submission ðŸ“¦","metadata":{}},{"cell_type":"markdown","source":"### Make Predictions","metadata":{}},{"cell_type":"code","source":"# make predictions\nX_submission = submission_imputed[features['continuous'] + features['categorical']]\ny_submission_pred = voting_pipeline.predict(X_submission)\n\n# formatting predictions for submission file output\nsubmission_df = pd.DataFrame({'id': submission_imputed['id'], 'x_e_out [-]': y_submission_pred})\ndisplay(submission_df.head())\n\n# saving predictions to .csv file for submission\nsubmission_df.to_csv('submission.csv', header=True, index=False)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}