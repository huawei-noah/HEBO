{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4458,"databundleVersionId":34321,"sourceType":"competition"}],"dockerImageVersionId":30017,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Insights","metadata":{}},{"cell_type":"markdown","source":"Lets start by visualising the datasets in pandas","metadata":{}},{"cell_type":"code","source":"test_set_full_raw = pd.read_csv(\"../input/sf-crime/test.csv.zip\", compression=\"zip\", index_col='Id')\ntrain_set_full_raw = pd.read_csv(\"../input/sf-crime/train.csv.zip\", compression=\"zip\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set_full_raw.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check what columns are present in the test set.\ntrain_set_full_raw.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will check if the train set contains any null values or missing rows. The data looks fine here so no data cleaning needed.","metadata":{}},{"cell_type":"code","source":"train_set_full_raw.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set_full_raw.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, looking at the summary of X, Y columns, there seems to be an outlier. Most of the Y falls around the 37 range but there is one at 90. Lets remove the 90 from our dataset","metadata":{}},{"cell_type":"code","source":"ts = train_set_full_raw.copy()\nts = ts[ts[\"Y\"]<90]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By plotting the X, Y coordinates we can see that the crimes cluster at certain regions of SF.","metadata":{}},{"cell_type":"code","source":"ts.plot(x=\"X\", y=\"Y\", kind=\"scatter\", alpha=0.01,figsize=(15,12))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also see the most common crime categories recorded and what are their most common resolutions","metadata":{}},{"cell_type":"code","source":"ts[\"Category\"].value_counts()","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Most common resolutions for each category in percentage\\n\")\nfor i in ts.groupby([\"Category\"])[\"Resolution\"]:\n  print('\\033[95m'+i[0]+'\\033[0m')\n  print(round(i[1].value_counts()[:3]/i[1].count()*100,1))\n  print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset gave us the dates of the crime. However, a more useful feature would be the hour the crime occured. Therefore, lets add it to our dataset.","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\n\nts[\"Hour\"] = ts.Dates.apply(lambda date_string: date_string[11:-6])\nts.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see which day of the week and hour crime occurs the most: ","metadata":{}},{"cell_type":"code","source":"ts.groupby([\"DayOfWeek\"])[\"Hour\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Now we can start preparing our dataset for training. We will only extract the relevant columns into our train and test set.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ntrain_full = ts.copy()\nX_train_full, y_train_full = np.array(train_full[[\"DayOfWeek\", \"PdDistrict\", \"X\", \"Y\", \"Hour\"]]), np.array(train_full[[\"Category\"]])\ny_train_full = y_train_full.ravel()\nX_test = test_set_full_raw.copy()\nX_test[\"Hour\"] = X_test.Dates.apply(lambda date_string: date_string[11:-6])\nX_test = X_test.drop(columns=[\"Dates\", \"Address\"])\nX_test = np.array(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We further split the full training set into train and validation set. We use Stratified sampling to ensure that the training and val set contains a proper representation of the categories present in the total population","metadata":{}},{"cell_type":"code","source":"import sklearn\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n\nfor train_index, test_index in sss.split(X_train_full, y_train_full):\n  X_train, y_train = X_train_full[train_index], y_train_full[train_index]\n  X_val, y_val = X_train_full[test_index], y_train_full[test_index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, lets set a pipeline to preprocess the datasets. StandardScaler() to normalise the numerical atttributes and OneHotEncoder() to convert the categorical attributes to arrays.","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = [2,3]\ncat_attribs = [0,1,4]\n\nnum_pipeline = Pipeline([\n                         ('std_scaler', StandardScaler())\n])\n\nfull_pipeline = ColumnTransformer([\n                                 ('num', num_pipeline, num_attribs),\n                                 ('cat', OneHotEncoder(), cat_attribs) \n])\n\nX_train_prepared = full_pipeline.fit_transform(X_train)\nX_val_prepared = full_pipeline.transform(X_val)\nX_test_prepared = full_pipeline.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Select and train model","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nimport xgboost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_prepared.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The train set contains 658 486 rows which lead to slow training time for me. Therefore, we will reduce the training set to 100 000 rows use Stratified Sampling again to get a good representation of the population.","metadata":{}},{"cell_type":"code","source":"ss = StratifiedShuffleSplit(n_splits=1, train_size=100_000, random_state=42)\nfor train_index, _ in ss.split(X_train_prepared, y_train):\n  X_train_prepared_small, y_train_small = X_train_prepared[train_index], y_train[train_index].ravel()\n\nX_train_prepared_small.shape, y_train_small.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ensemble Learning aggregates the prediction of a group of predictors. We usually get better results from it compared with just a single best individual predictor. \n\nFrom a prior, not very thorough, testing with different classifiers such as LinearSVC, BaggingClassifier, ExtraTreesClassifier. I found that Ensemble learning with XGBoost and RandomForest yield the best results.","metadata":{}},{"cell_type":"code","source":"rf_clf = RandomForestClassifier(max_depth=16, random_state=42, n_jobs=-1, verbose=3)\nxg_clf = xgboost.XGBClassifier()\n\nestimators = [\n            (\"rf\", rf_clf),\n            (\"xg\", xg_clf)\n]\n\nvoting_clf = VotingClassifier(estimators, n_jobs=-1, voting=\"soft\")\nvoting_clf.fit(X_train_prepared_small, y_train_small)\nvoting_clf.score(X_val_prepared, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, lets create the csv file for submission. This submission should give us a score of about 2.506:","metadata":{}},{"cell_type":"code","source":"y_pred = voting_clf.predict_proba(X_test_prepared)\npred_df = pd.DataFrame(y_pred, columns=[voting_clf.classes_])\npred_df[\"Id\"]= list(range(pred_df.shape[0]))\npred_df.to_csv(\"crime_pred_02.zip\", compression=\"zip\", index=False)\npred_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}